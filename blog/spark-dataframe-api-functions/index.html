<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>#8 DataFrame API#2: Columns, Expressions, and Functions | SimoneDangelo Blog</title>
<meta name=keywords content="coding,spark,tutorial"><meta name=description content="Most commonly used functions in Spark."><meta name=author content="Me"><link rel=canonical href=https://simdangelo.github.io/blog/spark-dataframe-api-functions/><link crossorigin=anonymous href=/assets/css/stylesheet.fc10a2e502c8379ca6fd0ccb1371d9d4049c0fd729db19019841464f9733f2ef.css integrity="sha256-/BCi5QLIN5ym/QzLE3HZ1AScD9cp2xkBmEFGT5cz8u8=" rel="preload stylesheet" as=style><link rel=icon href=https://simdangelo.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://simdangelo.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://simdangelo.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://simdangelo.github.io/apple-touch-icon.png><link rel=mask-icon href=https://simdangelo.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://simdangelo.github.io/blog/spark-dataframe-api-functions/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="#8 DataFrame API#2: Columns, Expressions, and Functions"><meta property="og:description" content="Most commonly used functions in Spark."><meta property="og:type" content="article"><meta property="og:url" content="https://simdangelo.github.io/blog/spark-dataframe-api-functions/"><meta property="og:image" content="https://simdangelo.github.io/images/papermod-cover.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-04-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-13T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://simdangelo.github.io/images/papermod-cover.png"><meta name=twitter:title content="#8 DataFrame API#2: Columns, Expressions, and Functions"><meta name=twitter:description content="Most commonly used functions in Spark."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://simdangelo.github.io/blog/"},{"@type":"ListItem","position":2,"name":"#8 DataFrame API#2: Columns, Expressions, and Functions","item":"https://simdangelo.github.io/blog/spark-dataframe-api-functions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"#8 DataFrame API#2: Columns, Expressions, and Functions","name":"#8 DataFrame API#2: Columns, Expressions, and Functions","description":"Most commonly used functions in Spark.","keywords":["coding","spark","tutorial"],"articleBody":"0. Resources: “Spark Essentials with Scala” course by Daniel Ciocîrlan (link here: https://rockthejvm.com/p/spark-essentials) https://sparkbyexamples.com/ 1. Start the Project If you’re interested in learning how to create a new Spark project in Scala, refer to the initial blog post on Spark available at the following link: https://simdangelo.github.io/blog/run-spark-application/. In this guide, we utilize the same project that was used in previous tutorials and will continue to use in future ones. You can download this project from the following repository: https://github.com/simdangelo/apache-spark-blog-tutorial.\n2. Basic functions 2.1. Select columns Selecting columns of a DataFrame is perhaps the most used operation is Spark.\nThe two functions that are used to do that are: .select() and selectExpr(). As you’ll see there are no practical reasons to choose one instead of the other, so feel free to use the one that you prefer. Whatever you choose, I suggest you to stay consistent with your choice in all your code to improve the readability. Just to say, .select() is by far the most used one.\nBefore diving into the syntax of each function, let’s describe briefly what these functions do internally and then we’ll represent them graphically. Remember that Spark DataFrames are split in partitions in between nodes and the cluster so, when I use a select function to select any number of columns from a DF, those columns are being selected on every partition on every node where the DF resides. After the selections, you will obtain a new DF with those columns and that will be reflected on every node in the cluster. As I described in one of the last posts in this blog, these functions represent a Transformation, specifically a Narrow Transformation:\nTransformation because it is an operation applied to an RDD that results in the creation of a new RDD and Spark does not immediately compute the result until an Action is called. Narrow because every partition in the original DF has exactly one corresponding output partition in the resulting DF. Let’s represent it graphically:\nSource: “Spark Essentials with Scala” course by Daniel Ciocîrlan (link here: https://rockthejvm.com/p/spark-essentials)\nNote: The technical term for a selection is Projection. So we are projecting the DF into a new DF. Projection is a term used from the theory of databases.\nLet’s download a csv file that we’ll use as example in this post: https://www.kaggle.com/datasets/kanchana1990/trending-ebay-watch-listings-2024 and put it in the src/main/resources/data folder in our Scala project.\nLet’s start with the usual configuration (read my other posts to know to do that) and then read the DF:\nval spark = SparkSession.builder() .appName(\"Columns, Expressions, and Functions\") .config(\"spark.master\", \"local[*]\") .getOrCreate() // read a DataFrame val df: DataFrame = spark.read .option(\"header\", \"true\") .option(\"delimiter\", \",\") .option(\"inferSchema\", \"true\") .csv(\"src/main/resources/data/ebay_watches.csv\") In addition, add these import declarations to make the following code work:\nimport org.apache.spark.sql.{DataFrame, SparkSession} import org.apache.spark.sql.functions._ import org.apache.spark.sql.types._ import org.apache.spark.sql.expressions.Window .select() There are several ways to select columns with this function:\n// select() function to select columns import org.apache.spark.sql.functions.{col, expr} import spark.implicits._ val select1 = df.select(\"itemLocation\",\"lastUpdated\", \"sold\") val select2 = df.select(expr(\"itemLocation\"), expr(\"lastUpdated\"), expr(\"sold\")) val select3 = df.select(col(\"itemLocation\"), col(\"lastUpdated\"), col(\"sold\")) val select4 = df.select($\"itemLocation\", $\"lastUpdated\", $\"sold\") val select5 = df.select(df(\"itemLocation\"), df(\"lastUpdated\"), df(\"sold\")) If we print apply .show() function to all these functions, we’ll see that all these expressions return the same result, that is:\nNote that:\nthe first two options are the most used ones; to use $\"\" expression you need to import spark.implicits._; to use col and .expr you need to import org.apache.spark.sql.functions.{col, expr}; in .expr() method you can pass a SQL-like string and expressions are a powerful constructs and they will allow you to process DF in almost any fashion that you like. Let’s make an example to understand the power of .expr().\nval df2 = df.select( col(\"itemLocation\"), col(\"sold\"), expr(\"sold * 2 as double_sold\"), expr(\"CASE WHEN itemLocation IS NULL THEN 'null_value' ELSE itemLocation END as itemLocation_mod\") ) Let’s apply .show() and here’s the result:\nTechnically, .select() function can take String (corresponding to select1 case) or Column (corresponding to select2, select3, select4 cases) types as arguments. It is worth mentioning because this aspect differs from .selectExpr() function.\n.selectExpr() This function is used for selecting and transforming columns using SQL expressions. Besides selecting columns, it allows you to use SQL expressions to manipulate columns (without creating a temporary table and views) including arithmetic operations, string manipulations, and aggregation functions.\nThis function doesn’t have a signature to take Column type, but it takes only String.\nYou can consider this function as a more powerful .expr() function. Let’s take the expression to create df2 in the previous paragraph and let’s rewrite it with .selectExpr() function:\nval df3 = df.selectExpr( \"itemLocation\", \"sold\", \"sold * 2 as double_sold\", \"CASE WHEN itemLocation IS NULL THEN 'null_value' ELSE itemLocation END as itemLocation_mod\" ) This expression for df3 returns the exact same result as df2.\n2.2. Create new columns .withColumn() Actually, we have already seen how to extends a DF with an additional column and create a new column with .select() function. Indeed, besides selecting existing column, this function allows to create new column.\nA more explicit function to create new columns is withColumn(). Let’s use both functions and we’ll see that the result is exactly identical:\nval df4 = df .select( col(\"sold\"), (col(\"sold\") * 2).alias(\"double_sold_v1\") // create column version 1 ) .withColumn(\"double_sold_v2\", col(\"sold\")*2) // create column version 2 Here’s the result:\n2.3. Evaluate conditions .when() This is one of the most utilised function in Spark. when() function is a conditional expression that evaluates a set of conditions and returns a corresponding value based on the first condition that evaluates to true. This condition must be followed by the .otherwise() function, which takes into account all those records that do not match the condition in the .when() function.\nLet’s say we want to fix the column seller because a specific seller has changed its name:\nval df5 = df .withColumn(\"seller_new\", when(col(\"seller\")===\"Direct Luxury\", lit(\"Luxury Boutique\")) .otherwise(col(\"seller\"))) Be careful: don’t forget the .otherwise() function, otherwise all records that don’t match the condition are set to NULL.\nHere’s the result:\nYou can also chain more than one conditions sequentially:\nval df5_2 = df .withColumn(\"seller_new\", when(col(\"seller\") === \"Direct Luxury\", lit(\"Luxury Boutique\")) .when(col(\"seller\") === \"WATCHGOOROO\", lit(\"Watch Gooroo\")) .when(col(\"seller\") === \"ioomobile\", lit(\"Ioo Mobile\")) .otherwise(col(\"seller\"))) 2.4. Rename columns .withColumnRenamed() The function .withColumnRenamed() takes two parameters:\nthe first one is the existing name the second one is the new name Actually we have already seen in the previous chapter a function to rename a column, that is .alias(). Even in this case both functions return the same result:\nval df6 = df.withColumnRenamed(\"itemLocation\", \"item_location\") val df6_2 = df.select(col(\"itemLocation\").alias(\"item_location\")) Of course in df6 all columns are selected and one is renamed, while in df6_2 only one column is selected and that one is renamed.\n2.5. Remove columns .drop() Remove a column with drop:\nval df7 = df.drop(\"availableText\", \"item_location\") 2.6. Filter records .filter() There are several ways to filter a DF with .filter() function (or equivalently with .where()):\nval df8 = df.filter(col(\"seller\")===\"Direct Luxury\") val df8_2 = df.where(col(\"seller\")=!=\"Direct Luxury\") val df8_3 = df.filter(\"seller = 'Direct Luxury'\") Note that you need to use === and =!= for column expressions because == and != are standard Scala Operators. You can of course whichever logical and arithmetical operators you want.\nYou can also extend the potentiality of filtering by using .isin() (if you want to include more than one element) and/or .not() (corresponding to ~ in PySpark) to deny a statement. For instance:\nval df8_4 = df.filter(col(\"seller\").isin(\"Direct Luxury\", \"Sigmatime\", \"ioomobile\")) val df8_5 = df.filter(not(col(\"seller\").isin(\"Direct Luxury\", \"Sigmatime\", \"ioomobile\"))) You can also filter all records that have a specific column with NULL values with .isNull (or the opposite with .isNotNull):\nval dfWithNull = df.filter(col(\"itemLocation\").isNull) val dfWithoutNull = df.filter(col(\"itemLocation\").isNotNull) The result of df8 is:\nYou can also chain filters:\n// chain filters val df9 = df.filter(col(\"seller\")===\"Direct Luxury\").filter(col(\"sold\")\u003e1) val df9_2 = df.filter((col(\"seller\")===\"Direct Luxury\") and (col(\"sold\")\u003e1)) val df9_3 = df.filter((col(\"seller\") === \"Direct Luxury\").and(col(\"sold\") \u003e 1)) val df9_4 = df.filter(\"seller = 'Direct Luxury' and sold \u003e 1\") You can also use or keyword, or symbols (\u0026\u0026 to use a boolean AND and || to use a boolean OR)\nNote that df9_3 uses and keyword without parenthesis because the and method is infix and this looks like more natural language.\n2.7. Order DataFrame rows .orderBy() .orderBy() function is pretty simple: it returns a new DF with rows ordered by one or more columns. By default the order is ascending:\nval orderBy = df .orderBy(col(\"sold\"), col(\"title\")) Here’s the result:\nBe careful:\nmake sure that columns are casted with the right data type. If sold column is string format, the order was alphabetically, while if it is numeric format the order is from the smallest to the biggest number;\nsince sold column contains NULL value, they appear first. If you want to keep the same order, but want NULLs to appear last, you can use asc_nulls_last function:\nval orderBy = df .orderBy(asc_nulls_last(\"sold\"), col(\"title\")) Now the result is:\nif you want to order in descending order you can use .desc(*col*) and .desc_nulls_last(*col*) if you want to deal with NULL:\nval orderBy_desc = df .orderBy(desc_nulls_last(\"sold\"), desc(\"title\")) 2.8. Summary statistics .min(), .max(), .avg() The usage of these functions is pretty trivial, so let’s make an example right now:\nval df_summary = df.select(min(\"sold\"), max(\"sold\"), avg(\"sold\")) Here’s the result:\nBe careful: these statistic summary functions must be applied only on int, long, double, and all the others numeric data types! So, before using them check if the schema is well applied. For instance, if sold had been a string, I could have circumvented the problem and used these functions by forcing the column to an integer (or other numeric data types ) with the .cast() function:\nval df_summary_casting = df.select(min(col(\"sold\").cast(\"int\")), max(col(\"sold\").cast(\"int\")), avg(col(\"sold\").cast(\"int\"))) 2.9. Union more DataFrames .union() To union two or more DFs with same number of columns you can use .union() function:\n// read another DataFrame val df_sales: DataFrame = spark.read .option(\"header\", \"true\") .option(\"delimiter\", \",\") .csv(\"src/main/resources/data/Walmart_sales.csv\") val df10 = df.select(\"availableText\").union(df_sales.select(\"Weekly_Sales\")) Be careful: the only requirements for .union() is that the numbers of columns of the two DFs is the same, but it’s not required to be the same ones. This function resolvers columns by name, meaning that the first column of df is appended to the first column of df_sales, the second column of df is appended to the first column of df_sales, etc., regardless their name. Only the position matters. This means that, like in the example above, I’m connecting two columns (availableText and Weekly_Sales) from two different DFs that have nothing to do with each other. If we don’t pay attention we risk causing damage.\n.unionByName() A safer approach is to use unionByName() function. This function resolver columns by name, meaning that the requirements is that the two DFs need to have the same columns, otherwise Spark return an error. But, if you want to union two DFs with different columns you can do:\nval df10_2 = df.unionByName(df_sales, allowMissingColumns = true) When using allowMissingColumns = true the result of the DataFrame contains NULL values for the columns that are missing on the DataFrame.\nThe result is:\nAs you can see, since there are no common columns between the two DFs, Spark add all the columns of df_sales that are missing on df.\n2.10. Convert string to data and timestamp types .to_date() and .to_timestamp() When we imported the csv file at the beginning of this post, we used .option(\"inferSchema\", \"true\"). As I suggested in one of the previous post in this blog, it’s not suggested to use this option, but instead to use .schema(csvSchema) to manually assign a data type to each column. In our particular case, the lastUpdated column contains timestamp values, but if we print the schema we see that Spark assigns a string data type to that column. So functions .to_date() and .to_timestamp() allow us to convert a column in a data or timestamp data type. Let’s convert lastUpdated into both date and timestamp columns:\nval df11 = df .withColumn(\"lastUpdated_timestamp\", to_timestamp(col(\"lastUpdated\"), \"MMM dd, yyyy HH:mm:ss z\")) .withColumn(\"lastUpdated_date\", to_date(col(\"lastUpdated\"), \"MMM dd, yyyy HH:mm:ss z\")) Note that both functions require the pattern format of the string you want to convert.\nThen let’s print the schema and the first 5 rows of df11 (we’ll print only the columns we are considering):\n2.11. Convert date or timestamp to string date_format() If you have a date or timestamp Column, you can convert it to a desirable string formats. Firstly, let’s create a new DF on which we’ll apply these functions:\nval orders = Seq( Row(\"sandwich\", \"big\", 10, \"2024-03-24\", \"2024-03-24:14:31:20\"), Row(\"pizza\", \"small\", 15, \"2024-03-22\", \"2024-03-22:21:00:12\") ) val schema = StructType(Seq( StructField(\"food\", StringType, nullable = true), StructField(\"size\", StringType, nullable = true), StructField(\"cost\", IntegerType, nullable = true), StructField(\"order_date\", StringType, nullable = true), StructField(\"order_timestamp\", StringType, nullable = true) )) val df_orders = spark.createDataFrame(spark.sparkContext.parallelize(orders), schema) val df_orders_clean = df_orders .withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\")) .withColumn(\"order_timestamp\", to_timestamp(col(\"order_timestamp\"), \"yyyy-MM-dd:HH:mm:ss\")) val df_date_formatted = df_orders_clean .withColumn(\"date_formatted\", date_format(col(\"order_date\"), \"ddMMMyyyy\")) .withColumn(\"timestamp_formatted\", date_format(col(\"order_timestamp\"), \"ddMMMyyyy HH:mm:ss\")) Of course you can use whichever string format you want.\nBe careful:\nthe original column must be date or timestamp column! the resulting columns are string format, no longer date or timestamp! Here’s the result:\n2.12. Extract a substring .substring() substring() function is used to extract the substring from a DataFrame string column by providing the position and length of the string you wanted to extract. Let’s say we want to change itemNumber column by extracting only the first 5 characters:\nval df12 = df .withColumn(\"itemNumber_v2\", substring(col(\"itemNumber\"), 0, 5)) Here’s the result:\n2.13. Remove white spaces .trim() Let’s create a new DF just to highlight the problems we want to solve with .trim() function:\nval orders_2 = Seq( (\" sandwich \", \"big\", 10, \"2024-03-24\"), (\" pizza\", \"small\", 15, \"2024-03-22\"), (\"salad \", \"small\", 15, \"2024-03-22\") ) val df13 = orders_2.toDF(\"food\", \"size\", \"cost\", \"order_date\") Note that food and size columns are not well formatted because values have some unwanted blank spaces at the beginning and at the end of values in some cases, and only at the beginning or only at the end in other cases. .trim() remove leading and trailing spaces:\nval df13_2 = df13 .withColumn(\"trim_food\", trim(col(\"food\"))) .withColumn(\"ltrim_food\", ltrim(col(\"food\"))) .withColumn(\"rtrim_food\", rtrim(col(\"food\"))) Note that in addition to .trim(), we also used .ltrim(), that removes only blank spaces at the beginning of the string, and .rtrim(), that removes blank spaces at the end of the string.\nHere’s the result:\n2.14. Pad a string .lpad() and .rpad() The .lpad() function used to left-pad a string column with a specified character or characters to reach a desired length. Considering that the maximum value of sold column is 1372 let’s say we want that all numbers must have 4 digits. This means that 1-digit numbers need three leading 0, 2-digit numbers need 2 leading zeros, 3-digit numbers need 1 leading zero:\nval df17 = df.withColumn(\"lpad_sold\", lpad(col(\"sold\"), 4, \"0\")) Here’s the result:\nThe functioning of .rpad() is exactly the same, but the pad happens to the right-side.\n2.15. Concatenate strings .concat() and .concat_ws() Let’s say you want to create a new column by concatenating two or more existing columns. I have two options:\nusing .concat(): concatenate two or more columns into a single new Column using .concat_ws(): concatenate two or more columns into a single new Column with a separator. Let’s write a code that, theoretically, should return the same result for each function:\nval tmp = Seq( (\"sandwich\", \"big\"), (\"pizza\", null), (null, \"small\"), (null, null) ) val df18 = tmp.toDF(\"food\", \"size\") val df18_2 = df18 .withColumn(\"concat\", concat(col(\"food\"), lit(\"_\"), col(\"size\"))) .withColumn(\"concat_ws\", concat_ws(\"_\", col(\"food\"), col(\"size\"))) Here’s the result:\nBe careful:\n.concat() and .concat_ws() are equivalent only if the columns to be concatenated are not-null. if one of the two columns contains a null value: .concat() produces a null value; .concat_ws() produces a not-null value considering only not-null columns; if both columns contain null values: .concat() produces a null value; .concat_ws() produces a an empty string. 2.16. Replace part of string with another string .regexp_replace() .regexp_replace() is a function that is used to replace part of a string (substring) value with another string on DataFrame column by using regular expression (regex). Let’s say I want to reaplace $ symbol with € symbol in priceWithCurrency column:\nval df19 = df .withColumn(\"dollars_to_euros\", regexp_replace(col(\"priceWithCurrency\"), \"US \\\\$\", \"EUR €\")) Note that we escape the $ character using double backslashes \\\\ because $ is a special character in regular expressions.\nHere’s the result:\nThis function can be used to remedy the problem we encountered in the paragraph 2.12: we can replace an empty string with a null value:\nval df18_final = df18_2 .withColumn(\"concat_ws_final\", when(col(\"concat_ws\")===\"\", lit(null)).otherwise(col(\"concat_ws\"))) Here’s the result:\n2.17. Create an array of strings .split() The .split() method returns a new Column object that represents an array of strings. Each element in the array is a substring of the original column that was split using the specified pattern. Of course you can also create other columns extracting elements from that list or, for instance, concatenate them:\nval df20 = df .withColumn(\"itemLocation_splitted\", split(col(\"itemLocation\"), \",\")) .withColumn(\"first_element\", split(col(\"itemLocation\"), \",\")(0)) .withColumn(\"second_element\", split(col(\"itemLocation\"), \",\")(1)) .withColumn(\"concat_elements\", concat_ws(\"_\", col(\"first_element\"), col(\"second_element\"))) Be careful: since , is the delimiter, the second_element values starts with a blank space, so in the concat_elements column we’ll have NY_ United States instead of NY_United States. To fix it, we can simply use .trim() function:\nval df20 = df .withColumn(\"itemLocation_splitted\", split(col(\"itemLocation\"), \",\")) .withColumn(\"first_element\", split(col(\"itemLocation\"), \",\")(0)) .withColumn(\"second_element\", split(col(\"itemLocation\"), \",\")(1)) .withColumn(\"second_element_fix\", trim(split(col(\"itemLocation\"), \",\")(1))) .withColumn(\"concat_elements\", concat_ws(\"_\", col(\"first_element\"), col(\"second_element\"))) .withColumn(\"concat_elements_fix\", concat_ws(\"_\", col(\"first_element\"), col(\"second_element_fix\"))) Here’s the result:\n3. Aggregation functions Aggregation functions in Apache Spark are used to perform calculations across multiple rows of a DataFrame, typically grouped by one or more columns. These functions help summarize or aggregate data in various ways, such as calculating counts, sums, averages, minimum and maximum values, and more.\nHere are some common aggregation functions in Spark:\ncount: Counts the number of rows in a DataFrame. countDistinct: Counts the number of distinct rows in a DataFrame. sum: Calculates the sum of numerical values in a column. avg: Calculates the average of numerical values in a column. min: Finds the minimum value in a column. max: Finds the maximum value in a column. agg: Allows for aggregating multiple functions simultaneously. Be careful: don’t forget to cast columns in the right way. Numerical aggregations work well only for numeric datatypes!\nGrouping columns is done with .groupBy() function. Let’s say we want to count how many records we have for each type Column value:\nval df21 = df .groupBy(\"type\").count() Here’s the result:\nLet’s complicate the exercise and let’s say we want now to know the min, max, and avg of priceWithCurrency column for each type. As we can note, priceWithCurrency column is a string Column in the form like US $2,054.83. We cannot use numerical aggregations unless we convert this string into a float type.\nval df21_2 = df .withColumn(\"price_fixed\", regexp_replace(regexp_replace(col(\"priceWithCurrency\"), \"US \\\\$\", \"\"), \"\\\\,\", \"\").cast(\"float\")) .groupBy(\"type\").agg(count(\"*\"), min(\"price_fixed\"), max(\"price_fixed\"), avg(\"price_fixed\")) Here’s the result:\nOf course you can also group by more than one column:\nval df21_3 = df .withColumn(\"price_fixed\", regexp_replace(regexp_replace(col(\"priceWithCurrency\"), \"US \\\\$\", \"\"), \"\\\\,\", \"\").cast(\"float\")) .groupBy(\"seller\", \"type\").agg(count(\"*\"), min(\"price_fixed\"), max(\"price_fixed\"), avg(\"price_fixed\")) Here’s the result:\n4. Advanced functions 4.1. Window functions Window functions in Spark are used to perform calculations across a set of rows related to the current row, often within a specified window of rows. These functions are applied to a group of rows defined by a partition and ordered by a specific column(s) within that partition. Let’s say we want to assign an ascending number starting from 1 to every partitions defined by the seller Column ordered by lastUpdated Column in descending order.\nThis means that:\nDataFrame will be split into partitions and each partition will contain all rows associated to each value of seller; rows in each partition are ordered in descending order according to lastUpdated Column; at each record a number starting from 1 is assigned. This operation will be obtained with .row_number() functions associated to a Window object carefully partitioned and ordered as decided above.\nLet’s make this example (remember we have first to cast lastUpdated to a timestamp column since now it’s a string column) by also including the functions .rank() and .dense_rank() for example purposes only:\nval windowSpec = Window.partitionBy(\"seller\").orderBy(desc(\"lastUpdated_timestamp\")) val df22 = df .withColumn(\"lastUpdated_timestamp\", to_timestamp(col(\"lastUpdated\"), \"MMM dd, yyyy HH:mm:ss z\")) .withColumn(\"row_number\", row_number().over(windowSpec)) .withColumn(\"rank\", rank().over(windowSpec)) .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) .orderBy(col(\"seller\"), desc(\"lastUpdated_timestamp\")) Here’s the result:\n4.2. .distinct() vs .dropDuplicates() These two functions are not really advanced functions, but I put them in this category because their use could be a little bit tricky (actually only .dropDuplicates() is tricky).\nBoth functions are used to remove duplicate rows from a DataFrame. The difference is that:\n.distinct() operates on all columns in the DF; .dropDuplicates() can operate both on entire DF or on a subset of columns, by specifying them as arguments in string format. Let’s make these examples and we’ll analyze one by one:\nval df12 = df.distinct() val df12_2 = df.select(\"type\").distinct() val df13 = df.dropDuplicates() val df13_2 = df.dropDuplicates(\"itemLocation\", \"seller\") df12 and df13 will be exactly the same.\ndf12_2 returns only the distinct elements of the column type:\ndf13_2 is the tricky one: .dropDuplicates() will only consider the itemLocation and seller columns when identifying duplicates, and it will keep a random occurrence of each duplicate. This is the tricky part. When the DF is split across partitions, you don’t know which occurrence Spark decides to keep (maybe there is an order, but honestly I didn’t find it). .dropDuplicates() keeps the first occurrence only if there is 1 partition. So be careful when using this function.\nWe can remedy this problem by using a window function, specifically a .row_number() function. What we have to do is force the sorting we want so that we take the specific record we are interested in and eliminate all other duplicates:\nval windowSpec2 = Window.partitionBy(\"itemLocation\", \"lastUpdated_timestamp\").orderBy(\"lastUpdated_timestamp\") val df_24_2_fix = df_clean .withColumn(\"row_number\", row_number().over(windowSpec2)) .filter(col(\"row_number\")===1) we created partitions by grouping by itemLocation and lastUpdated_timestamp, and ordering by lastUpdated_timestamp. In this way we forced the sorting so that we are sure not to remove duplicates randomly; to each record in each partition we assigned an ascending integer, in particular we assign 1 to each record we want to keep in the new DF; only records with a value of 1 are taken in each partition. ","wordCount":"3668","inLanguage":"en","image":"https://simdangelo.github.io/images/papermod-cover.png","datePublished":"2024-04-13T00:00:00Z","dateModified":"2024-04-13T00:00:00Z","author":[{"@type":"Person","name":"Me"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://simdangelo.github.io/blog/spark-dataframe-api-functions/"},"publisher":{"@type":"Organization","name":"SimoneDangelo Blog","logo":{"@type":"ImageObject","url":"https://simdangelo.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://simdangelo.github.io/ accesskey=h title="SimoneDangelo Blog (Alt + H)">SimoneDangelo Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://simdangelo.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://simdangelo.github.io/search/ title=Search><span>Search</span></a></li><li><a href=https://simdangelo.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/simdangelo title=GitHub><span>GitHub</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://simdangelo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://simdangelo.github.io/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">#8 DataFrame API#2: Columns, Expressions, and Functions</h1><div class=post-meta><span title='2024-04-13 00:00:00 +0000 UTC'>April 13, 2024</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-resources aria-label="0. Resources:">0. Resources:</a></li><li><a href=#1-start-the-project aria-label="1. Start the Project">1. Start the Project</a></li><li><a href=#2-basic-functions aria-label="2. Basic functions">2. Basic functions</a><ul><li><a href=#21-select-columns aria-label="2.1. Select columns">2.1. Select columns</a><ul><li><a href=#select aria-label=.select()>.select()</a></li><li><a href=#selectexpr aria-label=.selectExpr()>.selectExpr()</a></li></ul></li><li><a href=#22-create-new-columns-withcolumn aria-label="2.2. Create new columns .withColumn()">2.2. Create new columns .withColumn()</a></li><li><a href=#23-evaluate-conditions-when aria-label="2.3. Evaluate conditions .when()">2.3. Evaluate conditions .when()</a></li><li><a href=#24-rename-columns-withcolumnrenamed aria-label="2.4. Rename columns .withColumnRenamed()">2.4. Rename columns .withColumnRenamed()</a></li><li><a href=#25-remove-columns-drop aria-label="2.5. Remove columns .drop()">2.5. Remove columns .drop()</a></li><li><a href=#26-filter-records-filter aria-label="2.6. Filter records .filter()">2.6. Filter records .filter()</a></li><li><a href=#27-order-dataframe-rows-orderby aria-label="2.7. Order DataFrame rows .orderBy()">2.7. Order DataFrame rows .orderBy()</a></li><li><a href=#28-summary-statistics-min-max-avg aria-label="2.8. Summary statistics .min(), .max(), .avg()">2.8. Summary statistics .min(), .max(), .avg()</a></li><li><a href=#29-union-more-dataframes aria-label="2.9. Union more DataFrames">2.9. Union more DataFrames</a><ul><li><a href=#union aria-label=.union()>.union()</a></li><li><a href=#unionbyname aria-label=.unionByName()>.unionByName()</a></li></ul></li><li><a href=#210-convert-string-to-data-and-timestamp-types-to_date-and-to_timestamp aria-label="2.10. Convert string to data and timestamp types .to_date() and .to_timestamp()">2.10. Convert string to data and timestamp types .to_date() and .to_timestamp()</a></li><li><a href=#211-convert-date-or-timestamp-to-string-date_format aria-label="2.11. Convert date or timestamp to string date_format()">2.11. Convert date or timestamp to string date_format()</a></li><li><a href=#212-extract-a-substring-substring aria-label="2.12. Extract a substring .substring()">2.12. Extract a substring .substring()</a></li><li><a href=#213-remove-white-spaces-trim aria-label="2.13. Remove white spaces .trim()">2.13. Remove white spaces .trim()</a></li><li><a href=#214-pad-a-string-lpad-and-rpad aria-label="2.14. Pad a string .lpad() and .rpad()">2.14. Pad a string .lpad() and .rpad()</a></li><li><a href=#215-concatenate-strings-concat-and-concat_ws aria-label="2.15. Concatenate strings .concat() and .concat_ws()">2.15. Concatenate strings .concat() and .concat_ws()</a></li><li><a href=#216-replace-part-of-string-with-another-string-regexp_replace aria-label="2.16. Replace part of string with another string .regexp_replace()">2.16. Replace part of string with another string .regexp_replace()</a></li><li><a href=#217-create-an-array-of-strings-split aria-label="2.17. Create an array of strings .split()">2.17. Create an array of strings .split()</a></li></ul></li><li><a href=#3-aggregation-functions aria-label="3. Aggregation functions">3. Aggregation functions</a></li><li><a href=#4-advanced-functions aria-label="4. Advanced functions">4. Advanced functions</a><ul><li><a href=#41-window-functions aria-label="4.1. Window functions">4.1. Window functions</a></li><li><a href=#42-distinct-vs-dropduplicates aria-label="4.2. .distinct() vs .dropDuplicates()">4.2. .distinct() vs .dropDuplicates()</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=0-resources>0. Resources:<a hidden class=anchor aria-hidden=true href=#0-resources>#</a></h1><ul><li>“Spark Essentials with Scala” course by Daniel Ciocîrlan (link here: <a href=https://rockthejvm.com/p/spark-essentials>https://rockthejvm.com/p/spark-essentials</a>)</li><li><a href=https://sparkbyexamples.com/>https://sparkbyexamples.com/</a></li></ul><h1 id=1-start-the-project>1. Start the Project<a hidden class=anchor aria-hidden=true href=#1-start-the-project>#</a></h1><p>If you&rsquo;re interested in learning how to create a new Spark project in Scala, refer to the initial blog post on Spark available at the following link: <a href=https://simdangelo.github.io/blog/run-spark-application/>https://simdangelo.github.io/blog/run-spark-application/</a>. In this guide, we utilize the same project that was used in previous tutorials and will continue to use in future ones. You can download this project from the following repository: <a href=https://github.com/simdangelo/apache-spark-blog-tutorial>https://github.com/simdangelo/apache-spark-blog-tutorial</a>.</p><hr><h1 id=2-basic-functions>2. Basic functions<a hidden class=anchor aria-hidden=true href=#2-basic-functions>#</a></h1><h2 id=21-select-columns>2.1. Select columns<a hidden class=anchor aria-hidden=true href=#21-select-columns>#</a></h2><p><strong>Selecting columns of a DataFrame</strong> is perhaps the most used operation is Spark.</p><p>The two functions that are used to do that are: <code>.select()</code> and <code>selectExpr()</code>. As you’ll see there are no practical reasons to choose one instead of the other, so feel free to use the one that you prefer. Whatever you choose, I suggest you to stay consistent with your choice in all your code to improve the readability. Just to say, <code>.select()</code> is by far the most used one.</p><p>Before diving into the syntax of each function, let’s describe briefly what these functions do internally and then we’ll represent them graphically. Remember that Spark DataFrames are split in partitions in between nodes and the cluster so, when I use a select function to select any number of columns from a DF, those columns are being selected on every partition on every node where the DF resides. After the selections, you will obtain a new DF with those columns and that will be reflected on every node in the cluster. As I described in one of the last posts in this blog, these functions represent a Transformation, specifically a <strong>Narrow Transformation:</strong></p><ul><li><strong>Transformation</strong> because it is an operation applied to an RDD that results in the creation of a new RDD and Spark does not immediately compute the result until an <strong>Action</strong> is called.</li><li><strong>Narrow</strong> because every partition in the original DF has exactly one corresponding output partition in the resulting DF.</li></ul><p>Let’s represent it graphically:</p><p><img loading=lazy src=images/Untitled.png alt=Untitled></p><p><em>Source: “Spark Essentials with Scala” course by Daniel Ciocîrlan (link here: <a href=https://rockthejvm.com/p/spark-essentials>https://rockthejvm.com/p/spark-essentials</a>)</em></p><p>Note: The technical term for a selection is <strong>Projection</strong>. So we are projecting the DF into a new DF. Projection is a term used from the theory of databases.</p><p>Let’s download a <code>csv</code> file that we’ll use as example in this post: <a href=https://www.kaggle.com/datasets/kanchana1990/trending-ebay-watch-listings-2024>https://www.kaggle.com/datasets/kanchana1990/trending-ebay-watch-listings-2024</a> and put it in the <code>src/main/resources/data</code> folder in our Scala project.</p><p>Let’s start with the usual configuration (read my other posts to know to do that) and then read the DF:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> spark <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>SparkSession</span><span style=color:#f92672>.</span>builder<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>appName<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Columns, Expressions, and Functions&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>config<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.master&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;local[*]&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>getOrCreate<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// read a DataFrame
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>DataFrame</span> <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;header&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;delimiter&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;inferSchema&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>csv<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;src/main/resources/data/ebay_watches.csv&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>In addition, add these import declarations to make the following code work:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.spark.sql.<span style=color:#f92672>{</span><span style=color:#a6e22e>DataFrame</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>SparkSession</span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.spark.sql.functions._
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.spark.sql.types._
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> org.apache.spark.sql.expressions.Window
</span></span></code></pre></div><h3 id=select><code>.select()</code><a hidden class=anchor aria-hidden=true href=#select>#</a></h3><p>There are several ways to select columns with this function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#75715e>// select() function to select columns
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>import</span> org.apache.spark.sql.functions.<span style=color:#f92672>{</span>col<span style=color:#f92672>,</span> expr<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> spark.implicits._
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> select1 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>,</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> select2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>expr<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> expr<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>),</span> expr<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> select3 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> select4 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>$<span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>,</span> $<span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>,</span> $<span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> select5 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>df<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> df<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>),</span> df<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>If we print apply <code>.show()</code> function to all these functions, we’ll see that all these expressions return the same result, that is:</p><p><img loading=lazy src=images/Untitled%201.png alt=Untitled></p><p>Note that:</p><ul><li>the first two options are the most used ones;</li><li>to use <code>$""</code> expression you need to import <code>spark.implicits._</code>;</li><li>to use <code>col</code> and <code>.expr</code> you need to import <code>org.apache.spark.sql.functions.{col, expr}</code>;</li><li>in <code>.expr()</code> method you can pass a <strong>SQL-like string</strong> and expressions are a powerful constructs and they will allow you to process DF in almost any fashion that you like.</li></ul><p>Let’s make an example to understand the power of <code>.expr()</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  expr<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold * 2 as double_sold&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  expr<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;CASE WHEN itemLocation IS NULL THEN &#39;null_value&#39; ELSE itemLocation END as itemLocation_mod&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Let’s apply <code>.show()</code> and here’s the result:</p><p><img loading=lazy src=images/Untitled%202.png alt=Untitled></p><p>Technically, <code>.select()</code> function can take <code>String</code> (corresponding to <code>select1</code> case) or <code>Column</code> (corresponding to <code>select2</code>, <code>select3</code>, <code>select4</code> cases) types as arguments. It is worth mentioning because this aspect differs from <code>.selectExpr()</code> function.</p><h3 id=selectexpr><code>.selectExpr()</code><a hidden class=anchor aria-hidden=true href=#selectexpr>#</a></h3><p>This function is used for selecting and transforming columns using <strong>SQL expressions</strong>. Besides selecting columns, it allows you to use SQL expressions to manipulate columns (without creating a temporary table and views) including arithmetic operations, string manipulations, and aggregation functions.</p><p>This function doesn’t have a signature to take <code>Column</code> type, but it takes only <code>String</code>.</p><p>You can consider this function as a more powerful <code>.expr()</code> function. Let’s take the expression to create df2 in the previous paragraph and let’s rewrite it with <code>.selectExpr()</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df3 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>selectExpr<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;sold * 2 as double_sold&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;CASE WHEN itemLocation IS NULL THEN &#39;null_value&#39; ELSE itemLocation END as itemLocation_mod&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>This expression for <code>df3</code> returns the exact same result as <code>df2</code>.</p><h2 id=22-create-new-columns-withcolumn>2.2. Create new columns <code>.withColumn()</code><a hidden class=anchor aria-hidden=true href=#22-create-new-columns-withcolumn>#</a></h2><p>Actually, we have already seen how to extends a DF with an additional column and create a new column with <code>.select()</code> function. Indeed, besides selecting existing column, this function allows to create new column.</p><p>A more explicit function to create new columns is <code>withColumn()</code>. Let’s use both functions and we’ll see that the result is exactly identical:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df4 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>).</span>alias<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;double_sold_v1&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// create column version 1
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;double_sold_v2&#34;</span><span style=color:#f92672>,</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)*</span><span style=color:#ae81ff>2</span><span style=color:#f92672>)</span> <span style=color:#75715e>// create column version 2
</span></span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%203.png alt=Untitled></p><h2 id=23-evaluate-conditions-when>2.3. Evaluate conditions <code>.when()</code><a hidden class=anchor aria-hidden=true href=#23-evaluate-conditions-when>#</a></h2><p>This is one of the most utilised function in Spark. <code>when()</code> function is a conditional expression that evaluates a set of conditions and returns a corresponding value based on the first condition that evaluates to true. This condition must be followed by the <code>.otherwise()</code> function, which takes into account all those records that do not match the condition in the <code>.when()</code> function.</p><p>Let’s say we want to fix the column <code>seller</code> because a specific seller has changed its name:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df5 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller_new&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    when<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)===</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>,</span> lit<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Luxury Boutique&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>otherwise<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p><strong>Be careful</strong>: don’t forget the <code>.otherwise()</code> function, otherwise all records that don’t match the condition are set to <code>NULL</code>.</p><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%204.png alt=Untitled></p><p>You can also chain more than one conditions sequentially:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df5_2 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller_new&#34;</span><span style=color:#f92672>,</span>
</span></span><span style=display:flex><span>    when<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>,</span> lit<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Luxury Boutique&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>when<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#34;WATCHGOOROO&#34;</span><span style=color:#f92672>,</span> lit<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Watch Gooroo&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>when<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#34;ioomobile&#34;</span><span style=color:#f92672>,</span> lit<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Ioo Mobile&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>otherwise<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><h2 id=24-rename-columns-withcolumnrenamed>2.4. Rename columns <code>.withColumnRenamed()</code><a hidden class=anchor aria-hidden=true href=#24-rename-columns-withcolumnrenamed>#</a></h2><p>The function <code>.withColumnRenamed()</code> takes two parameters:</p><ul><li>the first one is the existing name</li><li>the second one is the new name</li></ul><p>Actually we have already seen in the previous chapter a function to rename a column, that is <code>.alias()</code>. Even in this case both functions return the same result:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df6 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>withColumnRenamed<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;item_location&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df6_2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>).</span>alias<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;item_location&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Of course in <code>df6</code> all columns are selected and one is renamed, while in <code>df6_2</code> only one column is selected and that one is renamed.</p><h2 id=25-remove-columns-drop>2.5. Remove columns <code>.drop()</code><a hidden class=anchor aria-hidden=true href=#25-remove-columns-drop>#</a></h2><p>Remove a column with <code>drop</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df7 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>drop<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;availableText&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;item_location&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><h2 id=26-filter-records-filter>2.6. Filter records <code>.filter()</code><a hidden class=anchor aria-hidden=true href=#26-filter-records-filter>#</a></h2><p>There are several ways to filter a DF with <code>.filter()</code> function (or equivalently with <code>.where()</code>):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df8 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)===</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df8_2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>where<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)=!=</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df8_3 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller = &#39;Direct Luxury&#39;&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Note that you need to use <code>===</code> and <code>=!=</code> for column expressions because <code>==</code> and <code>!=</code> are standard Scala Operators. You can of course whichever logical and arithmetical operators you want.</p><p>You can also extend the potentiality of filtering by using <code>.isin()</code> (if you want to include more than one element) and/or <code>.not()</code> (corresponding to <code>~</code> in PySpark) to deny a statement. For instance:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df8_4 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>).</span>isin<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;Sigmatime&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;ioomobile&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df8_5 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>not<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>).</span>isin<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;Sigmatime&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;ioomobile&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p>You can also filter all records that have a specific column with <code>NULL</code> values with <code>.isNull</code> (or the opposite with <code>.isNotNull</code>):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> dfWithNull <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>).</span>isNull<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> dfWithoutNull <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>).</span>isNotNull<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>The result of <code>df8</code> is:</p><p><img loading=lazy src=images/Untitled%205.png alt=Untitled></p><p>You can also chain filters:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#75715e>// chain filters
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df9 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)===</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>).</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)&gt;</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df9_2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>((</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)===</span><span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>)</span> and <span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)&gt;</span><span style=color:#ae81ff>1</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df9_3 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>((</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>===</span> <span style=color:#e6db74>&#34;Direct Luxury&#34;</span><span style=color:#f92672>).</span>and<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>)</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df9_4 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller = &#39;Direct Luxury&#39; and sold &gt; 1&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>You can also use or keyword, or symbols (<code>&&</code> to use a boolean AND and <code>||</code> to use a boolean OR)</p><p>Note that <code>df9_3</code> uses <code>and</code> keyword without parenthesis because the <code>and</code> method is <strong>infix</strong> and this looks like more natural language.</p><h2 id=27-order-dataframe-rows-orderby>2.7. Order DataFrame rows <code>.orderBy()</code><a hidden class=anchor aria-hidden=true href=#27-order-dataframe-rows-orderby>#</a></h2><p><code>.orderBy()</code> function is pretty simple: it returns a new DF with rows ordered by one or more columns. By default the order is ascending:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> orderBy <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;title&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%206.png alt=Untitled></p><p><strong>Be careful</strong>:</p><ul><li><p>make sure that columns are casted with the right data type. If <code>sold</code> column is string format, the order was alphabetically, while if it is numeric format the order is from the smallest to the biggest number;</p></li><li><p>since <code>sold</code> column contains <code>NULL</code> value, they appear first. If you want to keep the same order, but want <code>NULL</code>s to appear last, you can use <code>asc_nulls_last</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> orderBy <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span>asc_nulls_last<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;title&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Now the result is:</p><p><img loading=lazy src=images/Untitled%207.png alt=Untitled></p></li><li><p>if you want to order in descending order you can use <code>.desc(*col*)</code> and <code>.desc_nulls_last(*col*)</code> if you want to deal with <code>NULL</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> orderBy_desc <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span>desc_nulls_last<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span> desc<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;title&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div></li></ul><h2 id=28-summary-statistics-min-max-avg>2.8. Summary statistics <code>.min()</code>, <code>.max()</code>, <code>.avg()</code><a hidden class=anchor aria-hidden=true href=#28-summary-statistics-min-max-avg>#</a></h2><p>The usage of these functions is pretty trivial, so let’s make an example right now:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df_summary <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>min<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span> max<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span> avg<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%208.png alt=Untitled></p><p><strong>Be careful</strong>: these statistic summary functions must be applied only on <code>int</code>, <code>long</code>, <code>double</code>, and all the others numeric data types! So, before using them check if the schema is well applied. For instance, if <code>sold</code> had been a <code>string</code>, I could have circumvented the problem and used these functions by forcing the column to an <code>integer</code> (or other numeric data types ) with the <code>.cast()</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df_summary_casting <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span>min<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>).</span>cast<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;int&#34;</span><span style=color:#f92672>)),</span> max<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>).</span>cast<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;int&#34;</span><span style=color:#f92672>)),</span> avg<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>).</span>cast<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;int&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><h2 id=29-union-more-dataframes>2.9. Union more DataFrames<a hidden class=anchor aria-hidden=true href=#29-union-more-dataframes>#</a></h2><h3 id=union><code>.union()</code><a hidden class=anchor aria-hidden=true href=#union>#</a></h3><p>To union two or more DFs with same number of columns you can use <code>.union()</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#75715e>// read another DataFrame
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_sales<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>DataFrame</span> <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;header&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>option<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;delimiter&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>csv<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df10 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;availableText&#34;</span><span style=color:#f92672>).</span>union<span style=color:#f92672>(</span>df_sales<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Weekly_Sales&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p><strong>Be careful</strong>: the only requirements for <code>.union()</code> is that the numbers of columns of the two DFs is the same, but it’s not required to be the same ones. This function resolvers columns <strong>by name</strong>, meaning that the first column of <code>df</code> is appended to the first column of <code>df_sales</code>, the second column of <code>df</code> is appended to the first column of <code>df_sales</code>, etc., regardless their name. Only the position matters. This means that, like in the example above, I’m connecting two columns (<code>availableText</code> and <code>Weekly_Sales</code>) from two different DFs that have nothing to do with each other. If we don’t pay attention we risk causing damage.</p><h3 id=unionbyname><code>.unionByName()</code><a hidden class=anchor aria-hidden=true href=#unionbyname>#</a></h3><p>A safer approach is to use <code>unionByName()</code> function. This function resolver columns <strong>by name</strong>, meaning that the requirements is that the two DFs need to have the same columns, otherwise Spark return an error. But, if you want to union two DFs with different columns you can do:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df10_2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>unionByName<span style=color:#f92672>(</span>df_sales<span style=color:#f92672>,</span> allowMissingColumns <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>When using <code>allowMissingColumns = true</code> the result of the DataFrame contains <code>NULL</code> values for the columns that are missing on the DataFrame.</p><p>The result is:</p><p><img loading=lazy src=images/Untitled%209.png alt=Untitled></p><p>As you can see, since there are no common columns between the two DFs, Spark add all the columns of <code>df_sales</code> that are missing on <code>df</code>.</p><h2 id=210-convert-string-to-data-and-timestamp-types-to_date-and-to_timestamp>2.10. Convert string to data and timestamp types <code>.to_date()</code> and <code>.to_timestamp()</code><a hidden class=anchor aria-hidden=true href=#210-convert-string-to-data-and-timestamp-types-to_date-and-to_timestamp>#</a></h2><p>When we imported the csv file at the beginning of this post, we used <code>.option("inferSchema", "true")</code>. As I suggested in one of the previous post in this blog, it’s not suggested to use this option, but instead to use <code>.schema(csvSchema)</code> to manually assign a data type to each column. In our particular case, the <code>lastUpdated</code> column contains timestamp values, but if we print the schema we see that Spark assigns a string data type to that column. So functions <code>.to_date()</code> and <code>.to_timestamp()</code> allow us to convert a column in a data or timestamp data type. Let’s convert <code>lastUpdated</code> into both date and timestamp columns:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df11 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated_timestamp&#34;</span><span style=color:#f92672>,</span> to_timestamp<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;MMM dd, yyyy HH:mm:ss z&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated_date&#34;</span><span style=color:#f92672>,</span> to_date<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;MMM dd, yyyy HH:mm:ss z&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Note that both functions require the pattern format of the string you want to convert.</p><p>Then let’s print the schema and the first 5 rows of <code>df11</code> (we’ll print only the columns we are considering):</p><p><img loading=lazy src=images/Untitled%2010.png alt=Untitled></p><h2 id=211-convert-date-or-timestamp-to-string-date_format>2.11. Convert date or timestamp to string <code>date_format()</code><a hidden class=anchor aria-hidden=true href=#211-convert-date-or-timestamp-to-string-date_format>#</a></h2><p>If you have a date or timestamp Column, you can convert it to a desirable string formats. Firstly, let’s create a new DF on which we’ll apply these functions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> orders <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>Row</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sandwich&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;big&#34;</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>10</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-24&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-24:14:31:20&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>Row</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;pizza&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;small&#34;</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>15</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-22&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-22:21:00:12&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> schema <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>StructType</span><span style=color:#f92672>(</span><span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>StringType</span><span style=color:#f92672>,</span> nullable <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;size&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>StringType</span><span style=color:#f92672>,</span> nullable <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;cost&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>IntegerType</span><span style=color:#f92672>,</span> nullable <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_date&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>StringType</span><span style=color:#f92672>,</span> nullable <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_timestamp&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>StringType</span><span style=color:#f92672>,</span> nullable <span style=color:#66d9ef>=</span> <span style=color:#66d9ef>true</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_orders <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>(</span>spark<span style=color:#f92672>.</span>sparkContext<span style=color:#f92672>.</span>parallelize<span style=color:#f92672>(</span>orders<span style=color:#f92672>),</span> schema<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_orders_clean <span style=color:#66d9ef>=</span> df_orders
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_date&#34;</span><span style=color:#f92672>,</span> to_date<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_date&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;yyyy-MM-dd&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_timestamp&#34;</span><span style=color:#f92672>,</span> to_timestamp<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_timestamp&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;yyyy-MM-dd:HH:mm:ss&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_date_formatted <span style=color:#66d9ef>=</span> df_orders_clean
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;date_formatted&#34;</span><span style=color:#f92672>,</span> date_format<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_date&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;ddMMMyyyy&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;timestamp_formatted&#34;</span><span style=color:#f92672>,</span> date_format<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order_timestamp&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;ddMMMyyyy HH:mm:ss&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Of course you can use whichever string format you want.</p><p><strong>Be careful</strong>:</p><ul><li>the original column must be <code>date</code> or <code>timestamp</code> column!</li><li>the resulting columns are <code>string</code> format, no longer <code>date</code> or <code>timestamp</code>!</li></ul><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2011.png alt=Untitled></p><h2 id=212-extract-a-substring-substring>2.12. Extract a substring <code>.substring()</code><a hidden class=anchor aria-hidden=true href=#212-extract-a-substring-substring>#</a></h2><p><code>substring()</code> function is used to extract the substring from a DataFrame string column by providing the position and length of the string you wanted to extract. Let’s say we want to change <code>itemNumber</code> column by extracting only the first 5 characters:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df12 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemNumber_v2&#34;</span><span style=color:#f92672>,</span> substring<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemNumber&#34;</span><span style=color:#f92672>),</span> <span style=color:#ae81ff>0</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>5</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2012.png alt=Untitled></p><h2 id=213-remove-white-spaces-trim>2.13. Remove white spaces <code>.trim()</code><a hidden class=anchor aria-hidden=true href=#213-remove-white-spaces-trim>#</a></h2><p>Let’s create a new DF just to highlight the problems we want to solve with <code>.trim()</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> orders_2 <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#e6db74>&#34; sandwich  &#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;big&#34;</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>10</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-24&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;   pizza&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;small&#34;</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>15</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-22&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;salad  &#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;small&#34;</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>15</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;2024-03-22&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df13 <span style=color:#66d9ef>=</span> orders_2<span style=color:#f92672>.</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;size&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;cost&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;order_date&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Note that food and size columns are not well formatted because values have some unwanted blank spaces at the beginning and at the end of values in some cases, and only at the beginning or only at the end in other cases. <code>.trim()</code> remove leading and trailing spaces:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df13_2 <span style=color:#66d9ef>=</span> df13
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;trim_food&#34;</span><span style=color:#f92672>,</span> trim<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>)))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;ltrim_food&#34;</span><span style=color:#f92672>,</span> ltrim<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>)))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;rtrim_food&#34;</span><span style=color:#f92672>,</span> rtrim<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p>Note that in addition to <code>.trim()</code>, we also used <code>.ltrim()</code>, that removes only blank spaces at the beginning of the string, and <code>.rtrim()</code>, that removes blank spaces at the end of the string.</p><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2013.png alt=Untitled></p><h2 id=214-pad-a-string-lpad-and-rpad>2.14. Pad a string <code>.lpad()</code> and <code>.rpad()</code><a hidden class=anchor aria-hidden=true href=#214-pad-a-string-lpad-and-rpad>#</a></h2><p>The <code>.lpad()</code> function used to left-pad a string column with a specified character or characters to reach a desired length. Considering that the maximum value of sold column is 1372 let’s say we want that all numbers must have 4 digits. This means that 1-digit numbers need three leading 0, 2-digit numbers need 2 leading zeros, 3-digit numbers need 1 leading zero:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df17 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lpad_sold&#34;</span><span style=color:#f92672>,</span> lpad<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sold&#34;</span><span style=color:#f92672>),</span> <span style=color:#ae81ff>4</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;0&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2014.png alt=Untitled></p><p>The functioning of <code>.rpad()</code> is exactly the same, but the pad happens to the right-side.</p><h2 id=215-concatenate-strings-concat-and-concat_ws>2.15. Concatenate strings <code>.concat()</code> and <code>.concat_ws()</code><a hidden class=anchor aria-hidden=true href=#215-concatenate-strings-concat-and-concat_ws>#</a></h2><p>Let’s say you want to create a new column by concatenating two or more existing columns. I have two options:</p><ul><li>using <code>.concat()</code>: concatenate two or more columns into a single new Column</li><li>using <code>.concat_ws()</code>: concatenate two or more columns into a single new Column with a <strong>separator</strong>.</li></ul><p>Let’s write a code that, theoretically, should return the same result for each function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> tmp <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;sandwich&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;big&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#e6db74>&#34;pizza&#34;</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;small&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>(</span><span style=color:#66d9ef>null</span><span style=color:#f92672>,</span> <span style=color:#66d9ef>null</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df18 <span style=color:#66d9ef>=</span> tmp<span style=color:#f92672>.</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;size&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df18_2 <span style=color:#66d9ef>=</span> df18
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat&#34;</span><span style=color:#f92672>,</span> concat<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>),</span> lit<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;_&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;size&#34;</span><span style=color:#f92672>)))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_ws&#34;</span><span style=color:#f92672>,</span> concat_ws<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;_&#34;</span><span style=color:#f92672>,</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;food&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;size&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2015.png alt=Untitled></p><p><strong>Be careful</strong>:</p><ul><li><code>.concat()</code> and <code>.concat_ws()</code> are equivalent only if the columns to be concatenated are not-null.</li><li>if one of the two columns contains a null value:<ul><li><code>.concat()</code> produces a null value;</li><li><code>.concat_ws()</code> produces a not-null value considering only not-null columns;</li></ul></li><li>if both columns contain null values:<ul><li><code>.concat()</code> produces a null value;</li><li><code>.concat_ws()</code> produces a an empty string.</li></ul></li></ul><h2 id=216-replace-part-of-string-with-another-string-regexp_replace>2.16. Replace part of string with another string <code>.regexp_replace()</code><a hidden class=anchor aria-hidden=true href=#216-replace-part-of-string-with-another-string-regexp_replace>#</a></h2><p><code>.regexp_replace()</code> is a function that is used to replace part of a string (substring) value with another string on DataFrame column by using regular expression (regex). Let’s say I want to reaplace $ symbol with € symbol in <code>priceWithCurrency</code> column:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>  <span style=color:#66d9ef>val</span> df19 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>	  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;dollars_to_euros&#34;</span><span style=color:#f92672>,</span> regexp_replace<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;priceWithCurrency&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;US \\$&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;EUR €&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Note that we escape the <code>$</code> character using double backslashes <strong><code>\\</code></strong> because <code>$</code> is a special character in regular expressions.</p><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2016.png alt=Untitled></p><p>This function can be used to remedy the problem we encountered in the paragraph 2.12: we can replace an empty string with a null value:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df18_final <span style=color:#66d9ef>=</span> df18_2
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_ws_final&#34;</span><span style=color:#f92672>,</span> when<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_ws&#34;</span><span style=color:#f92672>)===</span><span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>,</span> lit<span style=color:#f92672>(</span><span style=color:#66d9ef>null</span><span style=color:#f92672>)).</span>otherwise<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_ws&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2017.png alt=Untitled></p><h2 id=217-create-an-array-of-strings-split>2.17. Create an array of strings <code>.split()</code><a hidden class=anchor aria-hidden=true href=#217-create-an-array-of-strings-split>#</a></h2><p>The <code>.split()</code> method returns a new Column object that represents an array of strings. Each element in the array is a substring of the original column that was split using the specified pattern. Of course you can also create other columns extracting elements from that list or, for instance, concatenate them:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df20 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation_splitted&#34;</span><span style=color:#f92672>,</span> split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;first_element&#34;</span><span style=color:#f92672>,</span> split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>0</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;second_element&#34;</span><span style=color:#f92672>,</span> split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_elements&#34;</span><span style=color:#f92672>,</span> concat_ws<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;_&#34;</span><span style=color:#f92672>,</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;first_element&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;second_element&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled%2018.png alt=Untitled></p><p><strong>Be careful</strong>: since <code>,</code> is the delimiter, the <code>second_element</code> values starts with a blank space, so in the concat_elements column we’ll have <code>NY_ United States</code> instead of <code>NY_United States</code>. To fix it, we can simply use <code>.trim()</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df20 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation_splitted&#34;</span><span style=color:#f92672>,</span> split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;first_element&#34;</span><span style=color:#f92672>,</span> split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>0</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;second_element&#34;</span><span style=color:#f92672>,</span> split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;second_element_fix&#34;</span><span style=color:#f92672>,</span> trim<span style=color:#f92672>(</span>split<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;,&#34;</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_elements&#34;</span><span style=color:#f92672>,</span> concat_ws<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;_&#34;</span><span style=color:#f92672>,</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;first_element&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;second_element&#34;</span><span style=color:#f92672>)))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;concat_elements_fix&#34;</span><span style=color:#f92672>,</span> concat_ws<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;_&#34;</span><span style=color:#f92672>,</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;first_element&#34;</span><span style=color:#f92672>),</span> col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;second_element_fix&#34;</span><span style=color:#f92672>)))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2019.png alt=Untitled></p><hr><h1 id=3-aggregation-functions>3. Aggregation functions<a hidden class=anchor aria-hidden=true href=#3-aggregation-functions>#</a></h1><p>Aggregation functions in Apache Spark are used to perform calculations across multiple rows of a DataFrame, typically grouped by one or more columns. These functions help summarize or aggregate data in various ways, such as calculating counts, sums, averages, minimum and maximum values, and more.</p><p>Here are some common aggregation functions in Spark:</p><ol><li><code>count</code>: Counts the number of rows in a DataFrame.</li><li><code>countDistinct</code>: Counts the number of distinct rows in a DataFrame.</li><li><code>sum</code>: Calculates the sum of numerical values in a column.</li><li><code>avg</code>: Calculates the average of numerical values in a column.</li><li><code>min</code>: Finds the minimum value in a column.</li><li><code>max</code>: Finds the maximum value in a column.</li><li><code>agg</code>: Allows for aggregating multiple functions simultaneously.</li></ol><p><strong>Be careful</strong>: don’t forget to cast columns in the right way. Numerical aggregations work well only for numeric datatypes!</p><p>Grouping columns is done with <code>.groupBy()</code> function. Let’s say we want to count how many records we have for each <code>type</code> Column value:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df21 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;type&#34;</span><span style=color:#f92672>).</span>count<span style=color:#f92672>()</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2020.png alt=Untitled></p><p>Let’s complicate the exercise and let’s say we want now to know the <code>min</code>, <code>max</code>, and <code>avg</code> of <code>priceWithCurrency</code> column for each <code>type</code>. As we can note, <code>priceWithCurrency</code> column is a <code>string</code> Column in the form like <code>US $2,054.83</code>. We cannot use numerical aggregations unless we convert this <code>string</code> into a <code>float</code> type.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df21_2 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>,</span> regexp_replace<span style=color:#f92672>(</span>regexp_replace<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;priceWithCurrency&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;US \\$&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;\\,&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>).</span>cast<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;float&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;type&#34;</span><span style=color:#f92672>).</span>agg<span style=color:#f92672>(</span>count<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;*&#34;</span><span style=color:#f92672>),</span> min<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>),</span> max<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>),</span> avg<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2021.png alt=Untitled></p><p>Of course you can also group by more than one column:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df21_3 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>,</span> regexp_replace<span style=color:#f92672>(</span>regexp_replace<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;priceWithCurrency&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;US \\$&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;\\,&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>).</span>cast<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;float&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;type&#34;</span><span style=color:#f92672>).</span>agg<span style=color:#f92672>(</span>count<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;*&#34;</span><span style=color:#f92672>),</span> min<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>),</span> max<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>),</span> avg<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;price_fixed&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2022.png alt=Untitled></p><hr><h1 id=4-advanced-functions>4. Advanced functions<a hidden class=anchor aria-hidden=true href=#4-advanced-functions>#</a></h1><h2 id=41-window-functions>4.1. Window functions<a hidden class=anchor aria-hidden=true href=#41-window-functions>#</a></h2><p>Window functions in Spark are used to perform calculations across a set of rows related to the current row, often within a specified window of rows. These functions are applied to a group of rows defined by a <strong>partition</strong> and <strong>ordered</strong> by a specific column(s) <strong>within that partition</strong>. Let’s say we want to assign an ascending number starting from 1 to every partitions defined by the <code>seller</code> Column ordered by <code>lastUpdated</code> Column in descending order.</p><p>This means that:</p><ul><li>DataFrame will be split into partitions and each partition will contain all rows associated to each value of <code>seller</code>;</li><li>rows in each partition are ordered in descending order according to <code>lastUpdated</code> Column;</li><li>at each record a number starting from 1 is assigned.</li></ul><p>This operation will be obtained with <code>.row_number()</code> functions associated to a <code>Window</code> object carefully partitioned and ordered as decided above.</p><p>Let’s make this example (remember we have first to cast <code>lastUpdated</code> to a <code>timestamp</code> column since now it’s a <code>string</code> column) by also including the functions <code>.rank()</code> and <code>.dense_rank()</code> for example purposes only:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> windowSpec <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Window</span><span style=color:#f92672>.</span>partitionBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>).</span>orderBy<span style=color:#f92672>(</span>desc<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated_timestamp&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df22 <span style=color:#66d9ef>=</span> df
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated_timestamp&#34;</span><span style=color:#f92672>,</span> to_timestamp<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;MMM dd, yyyy HH:mm:ss z&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;row_number&#34;</span><span style=color:#f92672>,</span> row_number<span style=color:#f92672>().</span>over<span style=color:#f92672>(</span>windowSpec<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;rank&#34;</span><span style=color:#f92672>,</span> rank<span style=color:#f92672>().</span>over<span style=color:#f92672>(</span>windowSpec<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;dense_rank&#34;</span><span style=color:#f92672>,</span> dense_rank<span style=color:#f92672>().</span>over<span style=color:#f92672>(</span>windowSpec<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>),</span> desc<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated_timestamp&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2023.png alt=Untitled></p><h2 id=42-distinct-vs-dropduplicates>4.2. <code>.distinct()</code> vs <code>.dropDuplicates()</code><a hidden class=anchor aria-hidden=true href=#42-distinct-vs-dropduplicates>#</a></h2><p>These two functions are not really advanced functions, but I put them in this category because their use could be a little bit tricky (actually only <code>.dropDuplicates()</code> is tricky).</p><p>Both functions are used to remove duplicate rows from a DataFrame. The difference is that:</p><ul><li><code>.distinct()</code> operates on <strong>all columns</strong> in the DF;</li><li><code>.dropDuplicates()</code> can operate both on <strong>entire DF</strong> or on a <strong>subset of columns</strong>, by specifying them as arguments in string format.</li></ul><p>Let’s make these examples and we’ll analyze one by one:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df12 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>distinct<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df12_2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>select<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;type&#34;</span><span style=color:#f92672>).</span>distinct<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df13 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>dropDuplicates<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df13_2 <span style=color:#66d9ef>=</span> df<span style=color:#f92672>.</span>dropDuplicates<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;seller&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><ul><li><p><code>df12</code> and <code>df13</code> will be exactly the same.</p></li><li><p><code>df12_2</code> returns only the distinct elements of the column <code>type</code>:</p><p><img loading=lazy src=images/Untitled%2024.png alt=Untitled></p></li><li><p><code>df13_2</code> is the tricky one: <code>.dropDuplicates()</code> will only consider the <code>itemLocation</code> and <code>seller</code> columns when identifying duplicates, and it will keep a <strong>random occurrence of each duplicate</strong>. This is the tricky part. When the DF is split across partitions, you don’t know which occurrence Spark decides to keep (maybe there is an order, but honestly I didn’t find it). <code>.dropDuplicates()</code> keeps the first occurrence only if there is 1 partition. So be careful when using this function.</p></li></ul><p>We can remedy this problem by using a window function, specifically a <code>.row_number()</code> function. What we have to do is force the sorting we want so that we take the specific record we are interested in and eliminate all other duplicates:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> windowSpec2 <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>Window</span><span style=color:#f92672>.</span>partitionBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;itemLocation&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;lastUpdated_timestamp&#34;</span><span style=color:#f92672>).</span>orderBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;lastUpdated_timestamp&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_24_2_fix <span style=color:#66d9ef>=</span> df_clean
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;row_number&#34;</span><span style=color:#f92672>,</span> row_number<span style=color:#f92672>().</span>over<span style=color:#f92672>(</span>windowSpec2<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;row_number&#34;</span><span style=color:#f92672>)===</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><ul><li>we created partitions by grouping by <code>itemLocation</code> and <code>lastUpdated_timestamp</code>, and ordering by <code>lastUpdated_timestamp</code>. In this way we forced the sorting so that we are sure not to remove duplicates randomly;</li><li>to each record in each partition we assigned an ascending integer, in particular we assign <code>1</code> to each record we want to keep in the new DF;</li><li>only records with a value of <code>1</code> are taken in each partition.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://simdangelo.github.io/tags/coding/>Coding</a></li><li><a href=https://simdangelo.github.io/tags/spark/>Spark</a></li><li><a href=https://simdangelo.github.io/tags/tutorial/>Tutorial</a></li></ul><nav class=paginav><a class=prev href=https://simdangelo.github.io/blog/spark-dataframe-join/><span class=title>« Prev</span><br><span>#9 DataFrame API#3: Join</span>
</a><a class=next href=https://simdangelo.github.io/blog/spark-dataframe-read-write/><span class=title>Next »</span><br><span>#7 DataFrame API#1: Read and Write DataFrames</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://simdangelo.github.io/>SimoneDangelo Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>