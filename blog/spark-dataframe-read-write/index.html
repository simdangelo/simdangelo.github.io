<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>#7 DataFrame API#1: Read and Write DataFrames | SimoneDangelo Blog</title>
<meta name="keywords" content="coding, spark, tutorial">
<meta name="description" content="Read and Write Spark DataFrames in several formats">
<meta name="author" content="Me">
<link rel="canonical" href="https://simdangelo.github.io/blog/spark-dataframe-read-write/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e4559b90ddde0017a53f9195be0e1bfa6c51510d85f13885c6fd5a54ee6029f8.css" integrity="sha256-5FWbkN3eABelP5GVvg4b&#43;mxRUQ2F8TiFxv1aVO5gKfg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://simdangelo.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://simdangelo.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://simdangelo.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://simdangelo.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://simdangelo.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://simdangelo.github.io/blog/spark-dataframe-read-write/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="#7 DataFrame API#1: Read and Write DataFrames" />
<meta property="og:description" content="Read and Write Spark DataFrames in several formats" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://simdangelo.github.io/blog/spark-dataframe-read-write/" />
<meta property="og:image" content="https://simdangelo.github.io/images/papermod-cover.png" />
<meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-03-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-30T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://simdangelo.github.io/images/papermod-cover.png" />
<meta name="twitter:title" content="#7 DataFrame API#1: Read and Write DataFrames"/>
<meta name="twitter:description" content="Read and Write Spark DataFrames in several formats"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://simdangelo.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "#7 DataFrame API#1: Read and Write DataFrames",
      "item": "https://simdangelo.github.io/blog/spark-dataframe-read-write/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "#7 DataFrame API#1: Read and Write DataFrames",
  "name": "#7 DataFrame API#1: Read and Write DataFrames",
  "description": "Read and Write Spark DataFrames in several formats",
  "keywords": [
    "coding", "spark", "tutorial"
  ],
  "articleBody": "In this article we’ll start to explore some basic functions belonging to Spark DataFrame API, in particular how to read and how to write Spark DataFrames.\nThis guide has been tested on macOS 14.3.1.\nYou can find the code used in this mini-guide (and in the next ones) in the following associated repository: https://github.com/simdangelo/apache-spark-blog-tutorial.\nNote that a section of this guide requires to spin up a Docker Container, so you have to install Docker Desktop on your machine.\n0. Resources: “Spark Essentials with Scala” course by Daniel Ciocîrlan (link here: https://rockthejvm.com/p/spark-essentials) Learning Spark: Lightning-Fast Data Analytics - Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee (Second Edition 2020, O’Reilly Media) coder2j Youtube Channel (https://www.youtube.com/watch?v=9MlZW9QeLM8\u0026t=29s\u0026ab_channel=coder2j) 1. SparkSession and SparkContext Let’s create a new Scala file by creating a new Object (as I explained in my previous post here https://simdangelo.github.io/blog/run-spark-application/) and we call it DFBasics1.\nThe first thing you have to do to start Spark Applications is to instantiate a SparkSession:\nval spark = SparkSession.builder() .appName(\"DataFrames Basics\") .config(\"spark.master\", \"local\") .getOrCreate() Without going too much into details, I’ll try to provide some hints on what SparkSession is, and why it replaced SparkContext since Spark 2.0.\nSo, let’s make an introduction of both and then we’ll consider the differences with respect to specific aspects:\nSparkSession: introduced in Spark 2.0 and it represents the unified entry point for interacting with Spark; combines functionalities of SparkContext, SQLContext, HiveContext, StreamingContext; supports multiple programming languages (Scala, Java, Python, R); offers higher-level API for structured data processing using DataFrames and Datasets APIs. supports structured querying using SQL or DataFrame and Dataset APIs; provides data source APIs, machine learning algorithms, and streaming capabilities. SparkContext: Represents the connection to a Spark cluster and it was the entry point in earlier versions of Spark (1.x). coordinates task execution across the cluster; core functionality for low-level programming and cluster interaction; creates RDDs; Summing up:\nSparkContext is low-level, while SparkSession is higher-level; SparkSession simplifies interaction and supports structured data processing; SparkSession is the recommended entry point for Spark Applications. 2. Read a DataFrame 2.1. What is a Spark DataFrame? Spark DataFrames are distributed in-memory collections of data, organized into tables with rows and columns and each table has a schema (meaning that column has a name and a specific data type [integer, string, array, etc.]).\nWe can schematically sum up the Spark DataFrame into some bullet points:\nit is a distributed collection of rows that are conforming to that schema; it is the schema, which is the description of the attribute of the data; think of a DataFrame as a table, which is split in between multiple nodes in a Spark Cluster. Let’s list some properties of Spark DataFrames (from now on I’ll call them as DFs).\nSince Spark DFs are basically tables conforming to a schema, it’s very important to know that these types are known to Spark when DF is being used (available at runtime), not a compile time (it’s possible to make them available at compile time with Dataset API). Spark DFs need to be distributed and that’s why Spark uses Partitioning. Partitioning means that the data is being split into files and shared between nodes in the cluster. Partitioning impact the processing parallelism of tour data. More partition, for example, may mean more parallelism. But if you have a thousand partitions and a single node to process them, the parallelism is 1 because you only have one node to process your data. Conversely, if you have one partition and a million of distributed nodes in your Spark Cluster, the processing parallelism will still be 1. The way we do partitioning is a big performance topic in Spark. DFs are immutable, meaning that they cannot be changed once created. If you want to modify them, you will create new DF via transformations. Speaking about transformations, there are a bunch of transformation types:\nnarrow transformation: one input partition contributes to at most one output partition (e.g. filter()) wide transformation: input partitions (one or more) creates many output partitions (e.g. orderBy()). These operations are known as shuffles, which are data exchanges between cluster nodes. Shuffling occurs in wide transformations and it’s a massive performance topic because it can impact the time that your job takes by orders of magnitude. Here’s a visual representation:\nSource: Learning Spark: Lightning-Fast Data Analytics - Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee (Second Edition 2020, O’Reilly Media)\nHow DFs work at runtime?\nOne of the most important mechanism in the whole Spark framework is the concept of Lazy Evaluation: Spark will wait until the last moment to execute the DFs transformations. This is because in complex projects it’s very usual to apply more sequential transformations to DFs, so Spark create a graph (called DAG, Directed Acyclic Graph), which is a sort of tree with all the transformations applied and it combines them in the best possible way to speed up the final computation.\nExample\nLet’s assume I want to read a table from a specific source (S3 Bucket), select only rows where country=='Italy' and store this new DF into a sink. The sequential operations are:\nread the entire table from the source; filter only the rows matching the filter; store the new DF into a sink. Since Spark wait until the last moment to execute the DFs transformations, it knows in advance every single step that it will have to take to store the new DF, so it creates a plan where it’s able to perform some optimizations. As regard the example above, Spark optimizes the operations by creating a new set of operations:\nread only the rows matching the filter from the source; store the new DF into a sink. It’s easy to understand that there’s no need to load the entire table from the source since I am only interested to specific records. Specifically, this kind of optimization is called Pushdown Filter.\nTalking about plans, there are actually 2 plans that Spark will compile:\nthe Logical Plan, which is the DF dependencies graph plus all the descriptions of the narrow and wide transformations that Spark will have to execute. So Spark knows in advance every single step. the Physical Plan, meaning that Spark will optimize the sequence of steps and it will know which node will execute which parts of the transformations. The last concept we have to know to understand how DFs work at runtime is the concept of action. An action represents very specific Spark functions applied on DFs (like .show(), .count(), .write(), etc.) that triggers the beginning of the Spark code. What does that mean? Earlier while talking about transformations, we said that “[…] Spark wait until the last moment to execute the DFs transformations […]”; so what is this “last moment”? Well, the “last moment” is represented by an action. So, as soon as Spark sees an action function, it will start the creation of the Logical and the Physical plans in order to create the new DF.\nThis is the power of Lazy Evaluation.\n2.2. Create a DataFrame by hand Let’s explore various solutions to create a DF by hand.\n.toDF()\ndef manualDF_v1(): Unit = { val orders = Seq( (\"sandwich\", \"big\", 10, \"2024-03-24\"), (\"pizza\", \"small\", 15, \"2024-03-22\") ) import spark.implicits._ val df = orders.toDF(\"food\", \"size\", \"cost\", \"order_date\") df.printSchema() df.show() } After calling this function in the main function of the Scala project, we’ll see in the console the result:\nThis method can be applied to a sequence of objects (Seq), but to access the toDF() method we have to import spark.implicits._.\nThe limitation of this method is that we have no control on the schema and the nullable flag.\ncreateDataFrame()\nThis method overcomes the limitations of the toDF() method because now we have control over complete schema customization:\ndef manualDF_v2(): Unit = { val orders = Seq( Row(\"sandwich\", \"big\", 10, \"2024-03-24\"), Row(\"pizza\", \"small\", 15, \"2024-03-22\") ) val schema = StructType(Seq( StructField(\"food\", StringType, nullable = true), StructField(\"size\", StringType, nullable = true), StructField(\"cost\", IntegerType, nullable = true), StructField(\"order_date\", StringType, nullable = true) )) val df = spark.createDataFrame(spark.sparkContext.parallelize(orders), schema) df.printSchema() df.show() } Note that now orders object is not anymore a Seq[(String, String, Int, String)], but it’s a Seq[Row]. Note that i was not able to cast order_date as DateType directly in the schema because otherwise Spark returns an error (I don’t know if it’s possible to do that in some other ways). It’s possible of course to cast to DateType after creating the DF. 2.3. Create a DataFrame from CSV file We will download a CSV file from Kaggle using the following link: https://www.kaggle.com/datasets/mikhail1681/walmart-sales and we will use this file as an example dataset. Let’s put this CSV file in src/main/resources/data path.\nThere are several solution to read a CSV file:\ndef readCsv_v1(): Unit = { val df = spark.read .format(\"csv\") .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .load(\"src/main/resources/data/Walmart_sales.csv\") df.show(5) } or alternatively:\ndef readCsv_v1(): Unit = { val df = spark.read .format(\"csv\") .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .option(\"path\", \"src/main/resources/data/Walmart_sales.csv\") .load() df.printSchema() df.show(5) } or alternatively (I prefer this last version):\ndef readCsv_v1(): Unit = { val df = spark.read .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .csv(\"src/main/resources/data/Walmart_sales.csv\") df.printSchema() df.show(5) } You can also specify the delimiter of the CSV file in case Spark is not able to recognize it:\ndef readCsv_v1(): Unit = { val df = spark.read .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .option(\"delimiter\", \",\") .csv(\"src/main/resources/data/Walmart_sales.csv\") df.printSchema() df.show(5) } The meaning of all the .options() is quite straightforward, so I’ll not explain them. The result is:\nNote that it’s not suggested to use .option(\"inferSchema\", \"true\") in production environment since we have no control on the schema.\nIndeed it’s always better to explicitly define the schema:\ndef readCsv_v2(): Unit = { val csvSchema = StructType( Array( StructField(\"Store\", StringType), StructField(\"Date\", DateType), StructField(\"Weekly_Sales\", DoubleType), StructField(\"Holiday_Flag\", IntegerType), StructField(\"Temperature\", DoubleType), StructField(\"Fuel_Price\", DoubleType), StructField(\"CPI\", DoubleType), StructField(\"Unemployment\", DoubleType) ) ) val df = spark.read .schema(csvSchema) .option(\"header\", \"true\") .option(\"dateFormat\", \"dd-MM-yyyy\") .csv(\"src/main/resources/data/Walmart_sales.csv\") df.printSchema() df.show(5) } Note that, if you cast a column as DateType, I suggested to set the option .option(\"dateFormat\", \"dd-MM-yyyy\") with the correct date format. Without it, I got weird results. The results is:\n2.4. Create a DataFrame from JSON file The principles are the same as the CSV case, so we’ll not spend too much effort on this.\nLet’s download a JSON file from Kaggle as example on this link https://www.kaggle.com/datasets/rtatman/iris-dataset-json-version and let’s put it into the same path of the CSV file:\ndef readJson(): Unit = { val jsonSchema = StructType( Array( StructField(\"sepalLength\", DoubleType), StructField(\"sepalWidth\", DoubleType), StructField(\"petalLength\", DoubleType), StructField(\"petalWidth\", DoubleType), StructField(\"species\", StringType), ) ) val df = spark.read .schema(jsonSchema) .json(\"src/main/resources/data/iris.json\") df.printSchema() df.show(5) } Here’s the result:\nOf course you can specify all the option() that we have seen in the previous CSV case.\nPS. I honestly don’t know why the first row is NULL.\n2.5. Create a DataFrame from Parquet file Parquet format file it is an open source, compressed, binary data storage format optimised for fast reading of columns and it works so well with Spark that Parquet is the default storage format for DF. One of the advantage is that it is very predictable, so you don’t need so many options that you had on CSVs.\nLet’s download a parquet file from this link https://www.tablab.app/datasets/sample/parquet and let’s put it into the src/main/resources/data/ path.\ndef readParquet(): Unit = { val df = spark.read .parquet(\"src/main/resources/data/MT cars.parquet\") df.printSchema() df.show(5) } 2.6. Create a DataFrame from a remote Database (Postgres) Reading from a remote DB and passing the data to Spark is a common pattern because to analyse Big Data there is often being migrated from databases to Spark.\nIn order to read from Postegres DB, we need to create it and insert some sample data. Let’s follow these steps:\nLet’s modify the build.sbt file by adding the PostgreSQL JDBC Driver:\nThisBuild / version := \"0.1.0-SNAPSHOT\" ThisBuild / scalaVersion := \"2.13.13\" lazy val root = (project in file(\".\")) .settings( name := \"spark-application-scala\" ) val sparkVersion = \"3.5.0\" val postgresVersion = \"42.6.0\" libraryDependencies ++= Seq( \"org.apache.spark\" %% \"spark-sql\" % sparkVersion, \"org.postgresql\" % \"postgresql\" % postgresVersion ) Now load the sbt changes clicking on the float button just appeared in the IDE.\nLet’s create a new folder in the root project folder called postgresDB and create a new file called init.sql:\n-- Create database DROP DATABASE IF EXISTS my_database; CREATE DATABASE my_database; -- Connect to the newly created database \\c my_database; -- Create table DROP TABLE IF EXISTS my_table; CREATE TABLE my_table ( id SERIAL PRIMARY KEY, first_name TEXT, last_name TEXT, gender TEXT, birth_date DATE ); -- Insert data into the table INSERT INTO my_table (first_name, last_name, gender, birth_date) VALUES ('John', 'Doe', 'M', '1990-03-01'); INSERT INTO my_table (first_name, last_name, gender, birth_date) VALUES ('Annah', 'Williams', 'F', '1980-08-15'); Create a docker-compose.yml file in the root project folder and put this code inside:\nversion: '2' services: postgres: image: postgres:latest container_name: postgres environment: - \"TZ=Europe/Amsterdam\" - \"POSTGRES_USER=docker\" - \"POSTGRES_PASSWORD=docker\" ports: - \"5432:5432\" volumes: - \"./postgresDB:/docker-entrypoint-initdb.d\" Go to the root project folder from the terminal and spin up the Postgres Docker Container:\ndocker compose up Now that the environment is correctly set, go back in the Spark application and write the Scala code:\ndef readPostgres(): Unit = { val df = spark.read .format(\"jdbc\") .option(\"driver\", \"org.postgresql.Driver\") .option(\"url\", \"jdbc:postgresql://localhost:5432/my_database\") .option(\"user\", \"docker\") .option(\"password\", \"docker\") .option(\"dbtable\", \"my_table\") .load() df.printSchema() df.show() } After calling this function from the main function, here’s the result you should see in the console:\n3. Write a DataFrame When writing DataFrame outputs, you have the flexibility to select from various formats such as JSON, CSV, TXT, Parquet, and more. While we’ll focus on the Parquet file here, as it’s commonly used, the same principles apply to all other formats with simple adjustments.\nLet’s read the CSV file Walmart_sales.csv in the data folder and let’s write it into both JSON and Parquet files into a new path:\ndef readCsv_file(): DataFrame = { val csvSchema = StructType( Array( StructField(\"Store\", StringType), StructField(\"Date\", DateType), StructField(\"Weekly_Sales\", DoubleType), StructField(\"Holiday_Flag\", IntegerType), StructField(\"Temperature\", DoubleType), StructField(\"Fuel_Price\", DoubleType), StructField(\"CPI\", DoubleType), StructField(\"Unemployment\", DoubleType) ) ) val df = spark.read .schema(csvSchema) .option(\"header\", \"true\") .option(\"dateFormat\", \"dd-MM-yyyy\") .csv(\"src/main/resources/data/Walmart_sales.csv\") df } def writeParquet(): Unit = { readCsv_file().write .mode(\"overwrite\") // or (SaveMode.Overwrite) .parquet(\"src/main/resources/data/output/walmart_sales.parquet\") } def writeJson(): Unit = { readCsv_file().write .mode(\"overwrite\") // or (SaveMode.Overwrite) .json(\"src/main/resources/data/output/walmart_sales.json\") } This is what we got by calling these two functions:\nThe interesting part is to compare the size of these two files so that we can understand how well Parquet file is optimzed (106 KB vs 995 KB):\nNote that it’s very likely that if you will save a big DF, you would see many more of these part-unique_identifiers.snappy.parquet or part-unique_identifiers.json files.\n",
  "wordCount" : "2414",
  "inLanguage": "en",
  "image": "https://simdangelo.github.io/images/papermod-cover.png","datePublished": "2024-03-30T00:00:00Z",
  "dateModified": "2024-03-30T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Me"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://simdangelo.github.io/blog/spark-dataframe-read-write/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "SimoneDangelo Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://simdangelo.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://simdangelo.github.io/" accesskey="h" title="SimoneDangelo Blog (Alt + H)">SimoneDangelo Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://simdangelo.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://simdangelo.github.io/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://simdangelo.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/simdangelo" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://simdangelo.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://simdangelo.github.io/blog/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      #7 DataFrame API#1: Read and Write DataFrames
    </h1>
    <div class="post-meta"><span title='2024-03-30 00:00:00 +0000 UTC'>March 30, 2024</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Me

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#0-resources" aria-label="0. Resources:">0. Resources:</a></li>
                <li>
                    <a href="#1-sparksession-and-sparkcontext" aria-label="1. SparkSession and SparkContext">1. SparkSession and SparkContext</a></li>
                <li>
                    <a href="#2-read-a-dataframe" aria-label="2. Read a DataFrame">2. Read a DataFrame</a><ul>
                        
                <li>
                    <a href="#21-what-is-a-spark-dataframe" aria-label="2.1. What is a Spark DataFrame?">2.1. What is a Spark DataFrame?</a></li>
                <li>
                    <a href="#22-create-a-dataframe-by-hand" aria-label="2.2. Create a DataFrame by hand">2.2. Create a DataFrame by hand</a></li>
                <li>
                    <a href="#23-create-a-dataframe-from-csv-file" aria-label="2.3. Create a DataFrame from CSV file">2.3. Create a DataFrame from CSV file</a></li>
                <li>
                    <a href="#24-create-a-dataframe-from-json-file" aria-label="2.4. Create a DataFrame from JSON file">2.4. Create a DataFrame from JSON file</a></li>
                <li>
                    <a href="#25-create-a-dataframe-from-parquet-file" aria-label="2.5. Create a DataFrame from Parquet file">2.5. Create a DataFrame from Parquet file</a></li>
                <li>
                    <a href="#26-create-a-dataframe-from-a-remote-database-postgres" aria-label="2.6. Create a DataFrame from a remote Database (Postgres)">2.6. Create a DataFrame from a remote Database (Postgres)</a></li></ul>
                </li>
                <li>
                    <a href="#3-write-a-dataframe" aria-label="3. Write a DataFrame">3. Write a DataFrame</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In this article we’ll start to explore some basic functions belonging to Spark <strong>DataFrame API</strong>, in particular how to <strong>read</strong> and how to <strong>write</strong> <strong>Spark DataFrames</strong>.</p>
<p>This guide has been tested on macOS 14.3.1.</p>
<p>You can find the code used in this mini-guide (and in the next ones) in the following associated repository: <a href="https://github.com/simdangelo/apache-spark-blog-tutorial">https://github.com/simdangelo/apache-spark-blog-tutorial</a>.</p>
<p>Note that a section of this guide requires to spin up a Docker Container, so you have to install Docker Desktop on your machine.</p>
<h1 id="0-resources">0. Resources:<a hidden class="anchor" aria-hidden="true" href="#0-resources">#</a></h1>
<ul>
<li>“Spark Essentials with Scala” course by Daniel Ciocîrlan (link here: <a href="https://rockthejvm.com/p/spark-essentials">https://rockthejvm.com/p/spark-essentials</a>)</li>
<li>Learning Spark: Lightning-Fast Data Analytics - Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee (Second Edition 2020, O&rsquo;Reilly Media)</li>
<li>coder2j Youtube Channel (<a href="https://www.youtube.com/watch?v=9MlZW9QeLM8&t=29s&ab_channel=coder2j">https://www.youtube.com/watch?v=9MlZW9QeLM8&amp;t=29s&amp;ab_channel=coder2j</a>)</li>
</ul>
<h1 id="1-sparksession-and-sparkcontext">1. SparkSession and SparkContext<a hidden class="anchor" aria-hidden="true" href="#1-sparksession-and-sparkcontext">#</a></h1>
<p>Let’s create a new Scala file by creating a new Object (as I explained in my previous post here <a href="https://simdangelo.github.io/blog/run-spark-application/">https://simdangelo.github.io/blog/run-spark-application/</a>) and we call it <code>DFBasics1</code>.</p>
<p>The first thing you have to do to start Spark Applications is to instantiate a <code>SparkSession</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">val</span> spark <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">SparkSession</span><span style="color:#f92672">.</span>builder<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;DataFrames Basics&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.master&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>getOrCreate<span style="color:#f92672">()</span>
</span></span></code></pre></div><p>Without going too much into details, I’ll try to provide some hints on what <code>SparkSession</code> is, and why it replaced <code>SparkContext</code> since Spark 2.0.</p>
<p>So, let’s make an introduction of both and then we’ll consider the differences with respect to specific aspects:</p>
<ul>
<li><code>SparkSession</code>:
<ul>
<li>introduced in Spark 2.0 and it represents the unified entry point for interacting with Spark;</li>
<li>combines functionalities of SparkContext, SQLContext, HiveContext, StreamingContext;</li>
<li>supports multiple programming languages (Scala, Java, Python, R);</li>
<li>offers higher-level API for structured data processing using <strong>DataFrames</strong> and <strong>Datasets APIs</strong>.</li>
<li>supports structured querying using SQL or DataFrame and Dataset APIs;</li>
<li>provides data source APIs, machine learning algorithms, and streaming capabilities.</li>
</ul>
</li>
<li><code>SparkContext</code>:
<ul>
<li>Represents the connection to a Spark cluster and it was the entry point in earlier versions of Spark (1.x).</li>
<li>coordinates task execution across the cluster;</li>
<li>core functionality for low-level programming and cluster interaction;</li>
<li>creates RDDs;</li>
</ul>
</li>
</ul>
<p>Summing up:</p>
<ul>
<li><code>SparkContext</code> is low-level, while <code>SparkSession</code> is higher-level;</li>
<li><code>SparkSession</code> simplifies interaction and supports structured data processing;</li>
<li><code>SparkSession</code> is the recommended entry point for Spark Applications.</li>
</ul>
<hr>
<h1 id="2-read-a-dataframe">2. Read a DataFrame<a hidden class="anchor" aria-hidden="true" href="#2-read-a-dataframe">#</a></h1>
<h2 id="21-what-is-a-spark-dataframe">2.1. What is a Spark DataFrame?<a hidden class="anchor" aria-hidden="true" href="#21-what-is-a-spark-dataframe">#</a></h2>
<p><strong>Spark DataFrames</strong> are <strong>distributed in-memory</strong> collections of data, organized into <strong>tables</strong> with <strong>rows</strong> and <strong>columns</strong> and each table has a <strong>schema</strong> (meaning that column has a name and a specific data type [integer, string, array, etc.]).</p>
<p>We can schematically sum up the Spark DataFrame into some bullet points:</p>
<ul>
<li>it is a <strong>distributed collection of rows</strong> that are conforming to that schema;</li>
<li>it is the <strong>schema</strong>, which is the description of the attribute of the data;</li>
<li>think of a DataFrame as a <strong>table</strong>, which is split in between <strong>multiple nodes in a Spark Cluster</strong>.</li>
</ul>
<p>Let’s list some properties of Spark DataFrames (from now on I’ll call them as <strong>DFs</strong>).</p>
<ul>
<li>Since Spark DFs are basically tables conforming to a <strong>schema</strong>, it’s very important to know that these types are known to Spark when DF is being used (<strong>available at runtime</strong>), not a compile time (it’s possible to make them available at compile time with Dataset API).</li>
<li>Spark DFs need to be distributed and that’s why Spark uses <strong>Partitioning</strong>. Partitioning means that the data is being split into files and shared between nodes in the cluster. Partitioning impact the processing parallelism of tour data. More partition, for example, may mean more parallelism. But if you have a thousand partitions and a single node to process them, the parallelism is 1 because you only have one node to process your data. Conversely, if you have one partition and a million of distributed nodes in your Spark Cluster, the processing parallelism will still be 1. The way we do partitioning is a big performance topic in Spark.</li>
<li>DFs are <strong>immutable</strong>, meaning that they cannot be changed once created. If you want to modify them, you will create new DF via <strong>transformations</strong>.</li>
</ul>
<p>Speaking about <strong>transformations</strong>, there are a bunch of transformation types:</p>
<ul>
<li><strong>narrow transformation</strong>: one input partition contributes to at most one output partition (e.g. <code>filter()</code>)</li>
<li><strong>wide transformation</strong>: input partitions (one or more) creates many output partitions (e.g. <code>orderBy()</code>). These operations are known as <strong>shuffles</strong>, which are data exchanges between cluster nodes. Shuffling occurs in wide transformations and it’s a massive performance topic because it can impact the time that your job takes by orders of magnitude.</li>
</ul>
<p>Here’s a visual representation:</p>
<p><img loading="lazy" src="images/Untitled.png" alt="Source: Learning Spark: Lightning-Fast Data Analytics - Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee (Second Edition 2020, O&amp;rsquo;Reilly Media)"  />
</p>
<p>Source: Learning Spark: Lightning-Fast Data Analytics - Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee (Second Edition 2020, O&rsquo;Reilly Media)</p>
<p><strong>How DFs work at runtime?</strong></p>
<p>One of the most important mechanism in the whole Spark framework is the concept of <strong>Lazy Evaluation</strong>: Spark will wait until the last moment to execute the DFs transformations. This is because in complex projects it’s very usual to apply more sequential transformations to DFs, so Spark create a graph (called <strong>DAG</strong>, <strong>Directed Acyclic Graph</strong>), which is a sort of tree with all the transformations applied and it combines them in the best possible way to speed up the final computation.</p>
<p><strong>Example</strong></p>
<p>Let’s assume I want to read a table from a specific source (S3 Bucket), select only rows where <code>country=='Italy'</code> and store this new DF into a sink. The sequential operations are:</p>
<ol>
<li>read the entire table from the source;</li>
<li>filter only the rows matching the filter;</li>
<li>store the new DF into a sink.</li>
</ol>
<p>Since Spark wait until the last moment to execute the DFs transformations, it knows in advance every single step that it will have to take to store the new DF, so it creates a <strong>plan</strong> where it’s able to perform some optimizations. As regard the example above, Spark optimizes the operations by creating a new set of operations:</p>
<ol>
<li>read only the rows matching the filter from the source;</li>
<li>store the new DF into a sink.</li>
</ol>
<p>It’s easy to understand that there’s no need to load the entire table from the source since I am only interested to specific records. Specifically, this kind of optimization is called <strong>Pushdown Filter</strong>.</p>
<p>Talking about <strong>plans</strong>, there are actually 2 plans that Spark will compile:</p>
<ul>
<li>the <strong>Logical Plan</strong>, which is the DF dependencies graph plus all the descriptions of the narrow and wide transformations that Spark will have to execute. So Spark knows in advance every single step.</li>
<li>the <strong>Physical Plan</strong>, meaning that Spark will optimize the sequence of steps and it will know which node will execute which parts of the transformations.</li>
</ul>
<p>The last concept we have to know to understand how DFs work at runtime is the concept of <strong>action</strong>. An action represents very specific Spark functions applied on DFs (like <code>.show()</code>, <code>.count()</code>, <code>.write()</code>, etc.) that triggers the beginning of the Spark code. What does that mean? Earlier while talking about transformations, we said that “[…] Spark wait until the last moment to execute the DFs transformations […]”; so what is this “last moment”? Well, the “last moment” is represented by an <strong>action</strong>. So, as soon as Spark sees an action function, it will start the creation of the Logical and the Physical plans in order to create the new DF.</p>
<p>This is the power of Lazy Evaluation.</p>
<h2 id="22-create-a-dataframe-by-hand">2.2. Create a DataFrame by hand<a hidden class="anchor" aria-hidden="true" href="#22-create-a-dataframe-by-hand">#</a></h2>
<p>Let’s explore various solutions to create a DF by hand.</p>
<ul>
<li>
<p><code>.toDF()</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> manualDF_v1<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> orders <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">Seq</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">(</span><span style="color:#e6db74">&#34;sandwich&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;big&#34;</span><span style="color:#f92672">,</span> <span style="color:#ae81ff">10</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;2024-03-24&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">(</span><span style="color:#e6db74">&#34;pizza&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;small&#34;</span><span style="color:#f92672">,</span> <span style="color:#ae81ff">15</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;2024-03-22&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">import</span> spark.implicits._
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> orders<span style="color:#f92672">.</span>toDF<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;food&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;size&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;cost&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;order_date&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>After calling this function in the <code>main</code> function of the Scala project, we’ll see in the console the result:</p>
<!-- raw HTML omitted -->
<p>This method can be applied to a <strong>sequence</strong> of objects (<code>Seq</code>), but to access the <code>toDF()</code> method we have to import <code>spark.implicits._</code><strong>.</strong></p>
<p>The limitation of this method is that we have no control on the schema and the nullable flag.</p>
</li>
<li>
<p><code>createDataFrame()</code></p>
<p>This method overcomes the limitations of the <code>toDF()</code> method because now we have control over complete schema customization:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> manualDF_v2<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> orders <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">Seq</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">Row</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;sandwich&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;big&#34;</span><span style="color:#f92672">,</span> <span style="color:#ae81ff">10</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;2024-03-24&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">Row</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;pizza&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;small&#34;</span><span style="color:#f92672">,</span> <span style="color:#ae81ff">15</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;2024-03-22&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> schema <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">StructType</span><span style="color:#f92672">(</span><span style="color:#a6e22e">Seq</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;food&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">StringType</span><span style="color:#f92672">,</span> nullable <span style="color:#66d9ef">=</span> <span style="color:#66d9ef">true</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;size&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">StringType</span><span style="color:#f92672">,</span> nullable <span style="color:#66d9ef">=</span> <span style="color:#66d9ef">true</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;cost&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">IntegerType</span><span style="color:#f92672">,</span> nullable <span style="color:#66d9ef">=</span> <span style="color:#66d9ef">true</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;order_date&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">StringType</span><span style="color:#f92672">,</span> nullable <span style="color:#66d9ef">=</span> <span style="color:#66d9ef">true</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>createDataFrame<span style="color:#f92672">(</span>spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>parallelize<span style="color:#f92672">(</span>orders<span style="color:#f92672">),</span> schema<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<ul>
<li>Note that now <code>orders</code> object is not anymore a  <code>Seq[(String, String, Int, String)]</code>, but it’s a  <code>Seq[Row]</code>.</li>
<li>Note that i was not able to cast <code>order_date</code> as <code>DateType</code> directly in the schema because otherwise Spark returns an error (I don’t know if it’s possible to do that in some other ways). It’s possible of course to cast to <code>DateType</code> after creating the DF.</li>
</ul>
</li>
</ul>
<h2 id="23-create-a-dataframe-from-csv-file">2.3. Create a DataFrame from CSV file<a hidden class="anchor" aria-hidden="true" href="#23-create-a-dataframe-from-csv-file">#</a></h2>
<p>We will download a CSV file from Kaggle using the following link: <a href="https://www.kaggle.com/datasets/mikhail1681/walmart-sales">https://www.kaggle.com/datasets/mikhail1681/walmart-sales</a> and we will use this file as an example dataset. Let’s put this CSV file in <code>src/main/resources/data</code> path.</p>
<p>There are several solution to read a CSV file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readCsv_v1<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;inferSchema&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>or alternatively:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readCsv_v1<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;inferSchema&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;path&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>or alternatively (I prefer this last version):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readCsv_v1<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>	  <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;inferSchema&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>csv<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>You can also specify the delimiter of the CSV file in case Spark is not able to recognize it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readCsv_v1<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>	  <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;inferSchema&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;delimiter&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;,&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>csv<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>	df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>The meaning of all the <code>.options()</code> is quite straightforward, so I’ll not explain them. The result is:</p>
<p><img loading="lazy" src="images/Untitled%203.png" alt="Untitled"  />
</p>
<p>Note that <strong>it’s not suggested to use</strong> <code>.option(&quot;inferSchema&quot;, &quot;true&quot;)</code> <strong>in production environment since we have no control on the schema</strong>.</p>
<p>Indeed it’s always better to explicitly define the <strong>schema</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readCsv_v2<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> csvSchema <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">StructType</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">Array</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Store&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">StringType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Date&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DateType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Weekly_Sales&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Holiday_Flag&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">IntegerType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Temperature&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Fuel_Price&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;CPI&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Unemployment&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>schema<span style="color:#f92672">(</span>csvSchema<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;dateFormat&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;dd-MM-yyyy&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>csv<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>Note that, if you cast a column as <code>DateType</code>, I suggested to set the option <code>.option(&quot;dateFormat&quot;, &quot;dd-MM-yyyy&quot;)</code> with the correct date format. Without it, I got weird results. The results is:</p>
<p><img loading="lazy" src="images/Untitled%204.png" alt="Untitled"  />
</p>
<h2 id="24-create-a-dataframe-from-json-file">2.4. Create a DataFrame from JSON file<a hidden class="anchor" aria-hidden="true" href="#24-create-a-dataframe-from-json-file">#</a></h2>
<p>The principles are the same as the CSV case, so we’ll not spend too much effort on this.</p>
<p>Let’s download a JSON file from Kaggle as example on this link <a href="https://www.kaggle.com/datasets/rtatman/iris-dataset-json-version">https://www.kaggle.com/datasets/rtatman/iris-dataset-json-version</a> and let’s put it into the same path of the CSV file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readJson<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> jsonSchema <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">StructType</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">Array</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;sepalLength&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;sepalWidth&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;petalLength&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;petalWidth&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;species&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">StringType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>schema<span style="color:#f92672">(</span>jsonSchema<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>json<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/iris.json&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>Here’s the result:</p>
<!-- raw HTML omitted -->
<p>Of course you can specify all the <code>option()</code> that we have seen in the previous CSV case.</p>
<p>PS. I honestly don’t know why the first row is <code>NULL</code>.</p>
<h2 id="25-create-a-dataframe-from-parquet-file">2.5. Create a DataFrame from Parquet file<a hidden class="anchor" aria-hidden="true" href="#25-create-a-dataframe-from-parquet-file">#</a></h2>
<p><strong>Parquet</strong> format file it is an open source, compressed, binary data storage format optimised for fast reading of columns and it works so well with Spark that Parquet is the default storage format for DF. One of the advantage is that it is very predictable, so you don’t need so many options that you had on CSVs.</p>
<p>Let’s download a parquet file from this link <a href="https://www.tablab.app/datasets/sample/parquet">https://www.tablab.app/datasets/sample/parquet</a> and let’s put it into the <code>src/main/resources/data/</code> path.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readParquet<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>parquet<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/MT cars.parquet&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">(</span><span style="color:#ae81ff">5</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p><img loading="lazy" src="images/Untitled%206.png" alt="Untitled"  />
</p>
<h2 id="26-create-a-dataframe-from-a-remote-database-postgres">2.6. Create a DataFrame from a remote Database (Postgres)<a hidden class="anchor" aria-hidden="true" href="#26-create-a-dataframe-from-a-remote-database-postgres">#</a></h2>
<p>Reading from a remote DB and passing the data to Spark is a common pattern because to analyse Big Data there is often being migrated from databases to Spark.</p>
<p>In order to read from Postegres DB, we need to create it and insert some sample data. Let’s follow these steps:</p>
<ul>
<li>
<p>Let&rsquo;s modify the <code>build.sbt</code> file by adding the PostgreSQL JDBC Driver:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ThisBuild / version :<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;0.1.0-SNAPSHOT&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ThisBuild / scalaVersion :<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;2.13.13&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lazy val root <span style="color:#f92672">=</span> <span style="color:#f92672">(</span>project in file<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;.&#34;</span><span style="color:#f92672">))</span>
</span></span><span style="display:flex;"><span>  .settings<span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    name :<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;spark-application-scala&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>val sparkVersion <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;3.5.0&#34;</span>
</span></span><span style="display:flex;"><span>val postgresVersion <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;42.6.0&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>libraryDependencies ++<span style="color:#f92672">=</span> Seq<span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;org.apache.spark&#34;</span> %% <span style="color:#e6db74">&#34;spark-sql&#34;</span> % sparkVersion,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;org.postgresql&#34;</span> % <span style="color:#e6db74">&#34;postgresql&#34;</span> % postgresVersion
</span></span><span style="display:flex;"><span><span style="color:#f92672">)</span>
</span></span></code></pre></div><p>Now load the sbt changes clicking on the float button just appeared in the IDE.</p>
</li>
<li>
<p>Let’s create a new folder in the root project folder called <code>postgresDB</code> and create a new file called <code>init.sql</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#75715e">-- Create database
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">DATABASE</span> <span style="color:#66d9ef">IF</span> <span style="color:#66d9ef">EXISTS</span> my_database;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">DATABASE</span> my_database;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">-- Connect to the newly created database
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#960050;background-color:#1e0010">\</span><span style="color:#66d9ef">c</span> my_database;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">-- Create table
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">TABLE</span> <span style="color:#66d9ef">IF</span> <span style="color:#66d9ef">EXISTS</span> my_table;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> my_table (
</span></span><span style="display:flex;"><span>    id SERIAL <span style="color:#66d9ef">PRIMARY</span> <span style="color:#66d9ef">KEY</span>,
</span></span><span style="display:flex;"><span>    first_name  TEXT,
</span></span><span style="display:flex;"><span>    last_name   TEXT,
</span></span><span style="display:flex;"><span>    gender      TEXT,
</span></span><span style="display:flex;"><span>    birth_date  DATE
</span></span><span style="display:flex;"><span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">-- Insert data into the table
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> my_table (first_name, last_name, gender, birth_date) <span style="color:#66d9ef">VALUES</span> (<span style="color:#e6db74">&#39;John&#39;</span>, <span style="color:#e6db74">&#39;Doe&#39;</span>, <span style="color:#e6db74">&#39;M&#39;</span>, <span style="color:#e6db74">&#39;1990-03-01&#39;</span>);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> my_table (first_name, last_name, gender, birth_date) <span style="color:#66d9ef">VALUES</span> (<span style="color:#e6db74">&#39;Annah&#39;</span>, <span style="color:#e6db74">&#39;Williams&#39;</span>, <span style="color:#e6db74">&#39;F&#39;</span>, <span style="color:#e6db74">&#39;1980-08-15&#39;</span>);
</span></span></code></pre></div></li>
<li>
<p>Create a <code>docker-compose.yml</code> file in the root project folder and put this code inside:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">version</span>: <span style="color:#e6db74">&#39;2&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">services</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">postgres</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">postgres:latest</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">container_name</span>: <span style="color:#ae81ff">postgres</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">environment</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;TZ=Europe/Amsterdam&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;POSTGRES_USER=docker&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;POSTGRES_PASSWORD=docker&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;5432:5432&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#e6db74">&#34;./postgresDB:/docker-entrypoint-initdb.d&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Go to the root project folder from the terminal and spin up the Postgres Docker Container:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker compose up
</span></span></code></pre></div></li>
</ul>
<p>Now that the environment is correctly set, go back in the Spark application and write the Scala code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readPostgres<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>format<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;jdbc&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;driver&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;org.postgresql.Driver&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;url&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;jdbc:postgresql://localhost:5432/my_database&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;user&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;docker&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;password&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;docker&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;dbtable&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;my_table&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>load<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>printSchema<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span>  df<span style="color:#f92672">.</span>show<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>After calling this function from the <code>main</code> function, here’s the result you should see in the console:</p>
<!-- raw HTML omitted -->
<hr>
<h1 id="3-write-a-dataframe">3. Write a DataFrame<a hidden class="anchor" aria-hidden="true" href="#3-write-a-dataframe">#</a></h1>
<p>When writing DataFrame outputs, you have the flexibility to select from various formats such as JSON, CSV, TXT, Parquet, and more. While we&rsquo;ll focus on the Parquet file here, as it&rsquo;s commonly used, the same principles apply to all other formats with simple adjustments.</p>
<p>Let’s read the CSV file Walmart_sales.csv in the <code>data</code> folder and let’s write it into both JSON and Parquet files into a new path:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> readCsv_file<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">DataFrame</span>  <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> csvSchema <span style="color:#66d9ef">=</span> <span style="color:#a6e22e">StructType</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">Array</span><span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Store&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">StringType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Date&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DateType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Weekly_Sales&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Holiday_Flag&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">IntegerType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Temperature&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Fuel_Price&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;CPI&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">StructField</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Unemployment&#34;</span><span style="color:#f92672">,</span> <span style="color:#a6e22e">DoubleType</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> df <span style="color:#66d9ef">=</span> spark<span style="color:#f92672">.</span>read
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>schema<span style="color:#f92672">(</span>csvSchema<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;header&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;true&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>option<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;dateFormat&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;dd-MM-yyyy&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>csv<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/Walmart_sales.csv&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  df
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> writeParquet<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  readCsv_file<span style="color:#f92672">().</span>write
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>mode<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;overwrite&#34;</span><span style="color:#f92672">)</span> <span style="color:#75715e">// or (SaveMode.Overwrite)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#f92672">.</span>parquet<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/output/walmart_sales.parquet&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> writeJson<span style="color:#f92672">()</span><span style="color:#66d9ef">:</span> <span style="color:#66d9ef">Unit</span> <span style="color:#f92672">=</span> <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  readCsv_file<span style="color:#f92672">().</span>write
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>mode<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;overwrite&#34;</span><span style="color:#f92672">)</span> <span style="color:#75715e">// or (SaveMode.Overwrite)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#f92672">.</span>json<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;src/main/resources/data/output/walmart_sales.json&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><p>This is what we got by calling these two functions:</p>
<p><img loading="lazy" src="images/Untitled%208.png" alt="Untitled"  />
</p>
<p>The interesting part is to compare the size of these two files so that we can understand how well Parquet file is optimzed (106 KB vs 995 KB):</p>
<p><img loading="lazy" src="images/Untitled%209.png" alt="Untitled"  />
</p>
<p><img loading="lazy" src="images/Untitled%2010.png" alt="Untitled"  />
</p>
<p>Note that it’s very likely that if you will save a big DF, you would see many more of these <strong>part-unique_identifiers.snappy.parquet</strong> or <strong>part-unique_identifiers.json</strong> files.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://simdangelo.github.io/tags/coding/">Coding</a></li>
      <li><a href="https://simdangelo.github.io/tags/spark/">Spark</a></li>
      <li><a href="https://simdangelo.github.io/tags/tutorial/">Tutorial</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://simdangelo.github.io/blog/presentation/">
    <span class="title">« Prev</span>
    <br>
    <span>#1 Presentation (pinned)</span>
  </a>
  <a class="next" href="https://simdangelo.github.io/blog/run-spark-application/">
    <span class="title">Next »</span>
    <br>
    <span>#6 How to run Spark Applications in Scala on your local machine</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://simdangelo.github.io/">SimoneDangelo Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
