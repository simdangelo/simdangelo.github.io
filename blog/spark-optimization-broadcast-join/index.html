<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LearningNoteSeries - SparkOptimization#1: Broadcast Joins | SimoneDangelo Blog</title>
<meta name=keywords content="coding,spark,tutorial"><meta name=description content="Read and Write Spark DataFrames in several formats."><meta name=author content="Me"><link rel=canonical href=https://simdangelo.github.io/blog/spark-optimization-broadcast-join/><link crossorigin=anonymous href=/assets/css/stylesheet.fc10a2e502c8379ca6fd0ccb1371d9d4049c0fd729db19019841464f9733f2ef.css integrity="sha256-/BCi5QLIN5ym/QzLE3HZ1AScD9cp2xkBmEFGT5cz8u8=" rel="preload stylesheet" as=style><link rel=icon href=https://simdangelo.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://simdangelo.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://simdangelo.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://simdangelo.github.io/apple-touch-icon.png><link rel=mask-icon href=https://simdangelo.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://simdangelo.github.io/blog/spark-optimization-broadcast-join/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="LearningNoteSeries - SparkOptimization#1: Broadcast Joins"><meta property="og:description" content="Read and Write Spark DataFrames in several formats."><meta property="og:type" content="article"><meta property="og:url" content="https://simdangelo.github.io/blog/spark-optimization-broadcast-join/"><meta property="og:image" content="https://simdangelo.github.io/images/papermod-cover.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-03-30T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-30T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://simdangelo.github.io/images/papermod-cover.png"><meta name=twitter:title content="LearningNoteSeries - SparkOptimization#1: Broadcast Joins"><meta name=twitter:description content="Read and Write Spark DataFrames in several formats."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://simdangelo.github.io/blog/"},{"@type":"ListItem","position":2,"name":"LearningNoteSeries - SparkOptimization#1: Broadcast Joins","item":"https://simdangelo.github.io/blog/spark-optimization-broadcast-join/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LearningNoteSeries - SparkOptimization#1: Broadcast Joins","name":"LearningNoteSeries - SparkOptimization#1: Broadcast Joins","description":"Read and Write Spark DataFrames in several formats.","keywords":["coding","spark","tutorial"],"articleBody":" 0. Resources “Spark Optimization with Scala” course by Daniel Ciocîrlan (link here: https://rockthejvm.com/p/spark-optimization) High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark (by Holden Karau, Rachel Warren) 1. Start the Project If you’re interested in learning how to create a new Spark project in Scala, refer to the initial blog post on Spark available at the following link: https://simdangelo.github.io/blog/run-spark-application/. In this guide, we utilize the same project that was used in previous tutorials and will continue to use in future ones. You can download this project from the following repository: https://github.com/simdangelo/apache-spark-blog-tutorial.\n2. Introduction As we already said in the last post specifically about joins, Joins are used to combine data from multiple DataFrames. In Spark, Joins are wide transformations. In order to compute a join, Spark scans the entire DataFrame across the entire cluster, so data is going to be moved around between various Spark nodes. So this involves shuffling, which is expensive for performance.\nNote: Managing joins requires different considerations between Spark Core and Spark SQL. Handling joins in Spark Core can be more challenging. For example, the ordering of operations is crucial since the DAG optimizer, unlike the SQL optimizer, cannot re-order operations or push down filters. Typically, we avoid using Spark Core because managing RDDs is not generally recommended. Instead, the Spark DataFrame API from the Spark SQL component is widely used. So, even tough with DataFrame API we’ll have less control on the low-level operations (and I’ll mention which these operations are in the next paragraphs), we’re happy to use it because the SQL Optimizer can help us a lot with automatic optimizations.\n3. Why are Joins Slow? Joins are expensive because they require that corresponding keys from each RDD (remember: a DataFrame is an RDD under the hood) are located at the same partition so that they can be combined locally.\nBefore exploring the main three scenarios we may encounter when joining two tables, let’s define what a Partitioner is because we need this concept to understand what will happen. A Partitioner is an abstraction that specifies the way data should be split or partitioned among the nodes in a cluster. If I’ll find enough information on this topic, I’ll dedicate a single post to it.\n3.1. Scenario 1: Shuffle Join The worst possible scenario is that both RDDs do not have a known partitioner. In this case a shuffle is needed because the rows with the same key need to be on the same partition on the same executor. Main characteristics:\nrows with the same key must be on the same partition. Spark needs to shuffle both RDDs. VERY expensive Shuffling is an expensive operation because of data transfer overhead and it may lead to:\npotential OOMs, because if some of the keys have a disproportionate amount of rows that will crash your executor because every single thing needs to be loaded in memory. this limits the parallelism, because Shuffle is a bottleneck to parallelism. Graphically:\nSource: High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark\n3.2. Scenario 2: Co-Partitioned RDDs Both RDDs have the same partitioner. Both RDDs are said to be Co-Partitioned.\nin order to run a join, you just need to fetch the existing partitions and do the join in memory no shuffles because it turns out to be a narrow dependency In terms of diagram:\nSource: High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark\nAs you can see, both RDD A and RDD B are partitioned in the exact same way and so the executors only need to load those partitions in memory and then do the join in memory.\n3.3. Scenario 3: Co-Located RDDs Both RDDs are not only co-partitioned, but the partitions are already load in memory on the executors. Both RDDs are said to be Co-Located.\nno partitions fetching no shuffle no network transfer fastest joins possible In terms of diagram:\nSource: High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark\n4. Broadcast Joins 4.1. Practical Usage Then let’s start with the usual configuration:\nval spark = SparkSession.builder() .appName(\"Broadcast Joins\") .master(\"local\") .config(\"spark.sql.adaptive.enabled\", \"false\") .getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") Note that I added .config(\"spark.sql.adaptive.enabled\", \"false\"). This is needed to disable an important optimization technique in Spark SQL, called Optimization Adaptive Query Execution (AQE), which is enabled by default since Apache Spark 3.2.0.\nLet’s suppose we want to make a classic join between a Small Table and a Big Table, that is a very common scenario (for instance when you want to fetch some definitions from the small table). So first of all, let’s create\nLet’s create a very big DF and a very small DF:\nval rows = sc.parallelize(List( Row(0, \"zero\"), Row(1, \"first\"), Row(2, \"second\"), Row(3, \"third\") )) val rowsSchema = StructType(Array( StructField(\"id\", IntegerType), StructField(\"order\", StringType) )) // small table val lookupTable: DataFrame = spark.createDataFrame(rows, rowsSchema) // large table val table = spark.range(1, 100000000) // column is \"id\" Now, let’s perform the join:\n// the classic join val joined = table.join(lookupTable, \"id\") Before running the code, let’s add a Thread.sleep() function so that we have time to see the SparkUI:\ndef main(args: Array[String]): Unit = { joined.explain joined.show() Thread.sleep(1000000) } Here’s the Spark UI:\nWe have a bunch of jobs triggered by this classic join. Let’s see the detail of the first step, just to see the complete DAG:\nFrom the SparkUI we can see that one of the DF is very tiny and is shuffled in the order of bytes, while the other is very big and is shuffled in the order of a half of a gigabyte. So we’re talking several orders of magnitude difference. Spark tries to optmize this by constructing multiple jobs with skipped stages, but it’s still not enough. Furthermore, and maybe most importantly, this job took about 17 seconds.\nLet’s go back in the code and let’s print the query plan in the console:\njoined.explain As we already know, the join took so long because both DF are being exchanged (the Exchange step). Besides these shuffles, there are also Sort operations for both DFs: the small one is very easy to sort but the large one not so much.\nSo we can do better.\n// a smarter join val joinedSmart = table.join(broadcast(lookupTable), \"id\") Then let’s trigger the DAG with a .show() action in the main function and let’s again show the physical plan:\nNotice that there is a step called BroadcastExchange that seems like a shuffle because it has the word “Exchange” in it, but it doesn’t involve a shuffle. Indeed it’s an exchange of data meaning that the DF is being copied to all the executors.\nThis Broadcast Join took less than a second (remember the “classic” join took about 17 seconds).\n4.2. Theoretical Explanation This job takes to little time because we’re employing a technique known as a Broadcast Join, where the smaller DF, instead of being shuffled causing the bigger DF to be shuffled as well, is being copied entirely to all the executors that are processing the join. So the bigger DF doesn’t need to be shuffled again because all the executors have the entire smaller DF to compare to.\nGraphically:\nThe situation when a Small DF and a Big Table join each other is so popular that Spark has implemented a broadcast join detection in the Catalyst Query Optimizer.\nLet’s see in the code this “auto-broadcast detection”.\nval bigTable = spark.range(1, 100000000) val smallTable = spark.range(1, 10000) // size estimated by Spark - auto-broadcast val joinedNumbers = bigTable.join(smallTable, \"id\") joinedNumbers.explain There is a BroadcastExchange with HashedRelationBroadcastMode, so the smaller DF is being broadcasted before being joined with the bigger one. So, Spark automatically detected that the small table is small enough and employs a BroadcastHashJoin itself. It happens even with the two tables swapped:\nval joinedNumbers = smallTable.join(bigTable, \"id\") Spark has an automatic range estimator or size estimator that is configured with:\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 30) // 30 bytes is the upper limit spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) // -1 to deactivate auto-broadcast If you put -1 you are deactivating auto-broadcast.\nThe default value for autoBroadcastJoinThreshold is 10 MB. This configuration is only available for DataFrame API, not for RDD API.\n","wordCount":"1355","inLanguage":"en","image":"https://simdangelo.github.io/images/papermod-cover.png","datePublished":"2024-03-30T00:00:00Z","dateModified":"2024-03-30T00:00:00Z","author":[{"@type":"Person","name":"Me"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://simdangelo.github.io/blog/spark-optimization-broadcast-join/"},"publisher":{"@type":"Organization","name":"SimoneDangelo Blog","logo":{"@type":"ImageObject","url":"https://simdangelo.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://simdangelo.github.io/ accesskey=h title="SimoneDangelo Blog (Alt + H)">SimoneDangelo Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://simdangelo.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://simdangelo.github.io/search/ title=Search><span>Search</span></a></li><li><a href=https://simdangelo.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/simdangelo title=GitHub><span>GitHub</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://simdangelo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://simdangelo.github.io/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">LearningNoteSeries - SparkOptimization#1: Broadcast Joins</h1><div class=post-meta><span title='2024-03-30 00:00:00 +0000 UTC'>March 30, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-resources aria-label="0. Resources">0. Resources</a></li><li><a href=#1-start-the-project aria-label="1. Start the Project">1. Start the Project</a></li><li><a href=#2-introduction aria-label="2. Introduction">2. Introduction</a></li><li><a href=#3-why-are-joins-slow aria-label="3. Why are Joins Slow?">3. Why are Joins Slow?</a><ul><li><a href=#31-scenario-1-shuffle-join aria-label="3.1. Scenario 1: Shuffle Join">3.1. Scenario 1: Shuffle Join</a></li><li><a href=#32-scenario-2-co-partitioned-rdds aria-label="3.2. Scenario 2: Co-Partitioned RDDs">3.2. Scenario 2: Co-Partitioned RDDs</a></li><li><a href=#33-scenario-3-co-located-rdds aria-label="3.3. Scenario 3: Co-Located RDDs">3.3. Scenario 3: Co-Located RDDs</a></li></ul></li><li><a href=#4-broadcast-joins aria-label="4. Broadcast Joins">4. Broadcast Joins</a><ul><li><a href=#41-practical-usage aria-label="4.1. Practical Usage">4.1. Practical Usage</a></li><li><a href=#42-theoretical-explanation aria-label="4.2. Theoretical Explanation">4.2. Theoretical Explanation</a></li></ul></li></ul></div></details></div><div class=post-content><hr><h1 id=0-resources>0. Resources<a hidden class=anchor aria-hidden=true href=#0-resources>#</a></h1><ul><li>“Spark Optimization with Scala” course by Daniel Ciocîrlan (link here: <a href=https://rockthejvm.com/p/spark-optimization>https://rockthejvm.com/p/spark-optimization</a>)</li><li>High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark (by Holden Karau, Rachel Warren)</li></ul><h1 id=1-start-the-project>1. Start the Project<a hidden class=anchor aria-hidden=true href=#1-start-the-project>#</a></h1><p>If you’re interested in learning how to create a new Spark project in Scala, refer to the initial blog post on Spark available at the following link: <a href=https://simdangelo.github.io/blog/run-spark-application/>https://simdangelo.github.io/blog/run-spark-application/</a>. In this guide, we utilize the same project that was used in previous tutorials and will continue to use in future ones. You can download this project from the following repository: <a href=https://github.com/simdangelo/apache-spark-blog-tutorial>https://github.com/simdangelo/apache-spark-blog-tutorial</a>.</p><h1 id=2-introduction>2. Introduction<a hidden class=anchor aria-hidden=true href=#2-introduction>#</a></h1><p>As we already said in the last post specifically about joins, <strong>Joins</strong> are used to <strong>combine data from multiple DataFrames.</strong> In Spark, Joins are <strong>wide transformations</strong>. In order to compute a join, Spark scans the entire DataFrame across the entire cluster, so data is going to be moved around between various Spark nodes. So this involves shuffling, which is <strong>expensive</strong> for performance.</p><p><strong>Note</strong>: Managing joins requires different considerations between <strong>Spark Core</strong> and <strong>Spark SQL</strong>. Handling joins in Spark Core can be more challenging. For example, the ordering of operations is crucial since the DAG optimizer, unlike the SQL optimizer, cannot re-order operations or push down filters. Typically, we avoid using Spark Core because managing RDDs is not generally recommended. Instead, the Spark DataFrame API from the Spark SQL component is widely used. So, even tough with DataFrame API we’ll have less control on the low-level operations (and I’ll mention which these operations are in the next paragraphs), we’re happy to use it because the SQL Optimizer can help us a lot with automatic optimizations.</p><h1 id=3-why-are-joins-slow>3. Why are Joins Slow?<a hidden class=anchor aria-hidden=true href=#3-why-are-joins-slow>#</a></h1><p>Joins are expensive because they require that corresponding keys from each RDD (remember: a DataFrame is an RDD under the hood) are located at the same partition so that they can be combined locally.</p><p>Before exploring the main three scenarios we may encounter when joining two tables, let’s define what a Partitioner is because we need this concept to understand what will happen. A <strong>Partitioner</strong> is an abstraction that specifies the way data should be split or partitioned among the nodes in a cluster. If I’ll find enough information on this topic, I’ll dedicate a single post to it.</p><h2 id=31-scenario-1-shuffle-join>3.1. Scenario 1: Shuffle Join<a hidden class=anchor aria-hidden=true href=#31-scenario-1-shuffle-join>#</a></h2><p>The worst possible scenario is that both <strong>RDDs do not have a known partitioner</strong>. In this case a <strong>shuffle</strong> is needed because the rows with the same key need to be on the same partition on the same executor. Main characteristics:</p><ul><li>rows with the same key must be on the same partition.</li><li>Spark needs to shuffle both RDDs.</li><li>VERY expensive</li></ul><p>Shuffling is an expensive operation because of data transfer overhead and it may lead to:</p><ul><li>potential OOMs, because if some of the keys have a disproportionate amount of rows that will crash your executor because every single thing needs to be loaded in memory.</li><li>this limits the parallelism, because Shuffle is a bottleneck to parallelism.</li></ul><p>Graphically:</p><p><img loading=lazy src=images/Untitled.png alt=Untitled>
<em>Source: High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark</em></p><h2 id=32-scenario-2-co-partitioned-rdds>3.2. Scenario 2: <strong>Co-Partitioned RDDs</strong><a hidden class=anchor aria-hidden=true href=#32-scenario-2-co-partitioned-rdds>#</a></h2><p>Both RDDs have the <strong>same partitioner</strong>. Both RDDs are said to be <strong>Co-Partitioned</strong>.</p><ul><li>in order to run a join, you just need to fetch the existing partitions and do the join in memory</li><li>no shuffles because it turns out to be a narrow dependency</li></ul><p>In terms of diagram:</p><p><img loading=lazy src=images/Untitled%201.png alt=Untitled>
<em>Source: High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark</em></p><p>As you can see, both RDD A and RDD B are partitioned in the exact same way and so the executors only need to load those partitions in memory and then do the join in memory.</p><h2 id=33-scenario-3-co-located-rdds>3.3. Scenario 3: <strong>Co-Located RDDs</strong><a hidden class=anchor aria-hidden=true href=#33-scenario-3-co-located-rdds>#</a></h2><p>Both RDDs are not only <strong>co-partitioned</strong>, but the partitions are already load in memory on the executors. Both RDDs are said to be <strong>Co-Located</strong>.</p><ul><li>no partitions fetching</li><li>no shuffle</li><li>no network transfer</li><li>fastest joins possible</li></ul><p>In terms of diagram:</p><p><img loading=lazy src=images/Untitled%202.png alt=Untitled>
<em>Source: High Performance Spark: Best Practices for Scaling and Optimizing Apache Spark</em></p><h1 id=4-broadcast-joins>4. Broadcast Joins<a hidden class=anchor aria-hidden=true href=#4-broadcast-joins>#</a></h1><h2 id=41-practical-usage>4.1. Practical Usage<a hidden class=anchor aria-hidden=true href=#41-practical-usage>#</a></h2><p>Then let’s start with the usual configuration:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> spark <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>SparkSession</span><span style=color:#f92672>.</span>builder<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>appName<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Broadcast Joins&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>master<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;local&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>config<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;false&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>getOrCreate<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sparkContext<span style=color:#f92672>.</span>setLogLevel<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;WARN&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Note that I added <code>.config("spark.sql.adaptive.enabled", "false")</code>. This is needed to disable an important optimization technique in Spark SQL, called Optimization Adaptive Query Execution (AQE), which is enabled by default since Apache Spark 3.2.0.</p><p>Let’s suppose we want to make a classic join between a <strong>Small Table</strong> and a <strong>Big Table</strong>, that is a very common scenario (for instance when you want to fetch some definitions from the small table). So first of all, let’s create</p><p>Let’s create a very big DF and a very small DF:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> rows <span style=color:#66d9ef>=</span> sc<span style=color:#f92672>.</span>parallelize<span style=color:#f92672>(</span><span style=color:#a6e22e>List</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Row</span><span style=color:#f92672>(</span><span style=color:#ae81ff>0</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;zero&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Row</span><span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;first&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Row</span><span style=color:#f92672>(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;second&#34;</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>Row</span><span style=color:#f92672>(</span><span style=color:#ae81ff>3</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;third&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> rowsSchema <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>StructType</span><span style=color:#f92672>(</span><span style=color:#a6e22e>Array</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;id&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>IntegerType</span><span style=color:#f92672>),</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>StructField</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;order&#34;</span><span style=color:#f92672>,</span> <span style=color:#a6e22e>StringType</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// small table
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> lookupTable<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>DataFrame</span> <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>(</span>rows<span style=color:#f92672>,</span> rowsSchema<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// large table
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> table <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>range<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>100000000</span><span style=color:#f92672>)</span> <span style=color:#75715e>// column is &#34;id&#34;
</span></span></span></code></pre></div><p>Now, let’s perform the join:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#75715e>// the classic join
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> joined <span style=color:#66d9ef>=</span> table<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>lookupTable<span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;id&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Before running the code, let’s add a <code>Thread.sleep()</code> function so that we have time to see the SparkUI:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>])</span><span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Unit</span> <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>	joined<span style=color:#f92672>.</span>explain
</span></span><span style=display:flex><span>	joined<span style=color:#f92672>.</span>show<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>Thread</span><span style=color:#f92672>.</span>sleep<span style=color:#f92672>(</span><span style=color:#ae81ff>1000000</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><p>Here’s the Spark UI:</p><p><img loading=lazy src=images/Untitled%203.png alt=Untitled></p><p>We have a bunch of jobs triggered by this classic join. Let’s see the detail of the first step, just to see the complete DAG:</p><p><img loading=lazy src=images/Untitled%204.png alt=Untitled></p><p>From the SparkUI we can see that one of the DF is very tiny and is shuffled in the order of bytes, while the other is very big and is shuffled in the order of a half of a gigabyte. So we’re talking several orders of magnitude difference. Spark tries to optmize this by constructing multiple jobs with skipped stages, but it’s still not enough. Furthermore, and maybe most importantly, this job took about <strong>17 seconds</strong>.</p><p>Let’s go back in the code and let’s print the query plan in the console:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>joined<span style=color:#f92672>.</span>explain
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled%205.png alt=Untitled></p><p>As we already know, the join took so long because both DF are being exchanged (the <code>Exchange</code> step). Besides these shuffles, there are also <code>Sort</code> operations for both DFs: the small one is very easy to sort but the large one not so much.</p><p>So we can do better.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#75715e>// a smarter join
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> joinedSmart <span style=color:#66d9ef>=</span> table<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>broadcast<span style=color:#f92672>(</span>lookupTable<span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;id&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Then let’s trigger the DAG with a .show() action in the main function and let’s again show the physical plan:</p><p><img loading=lazy src=images/Untitled%206.png alt=Untitled></p><p>Notice that there is a step called <code>BroadcastExchange</code> that seems like a shuffle because it has the word “Exchange” in it, but it <strong>doesn’t involve a shuffle</strong>. Indeed it’s an exchange of data meaning that the DF is being copied to all the executors.</p><p><img loading=lazy src=images/Untitled%207.png alt=Untitled></p><p>This <strong>Broadcast Join</strong> took <strong>less than a second</strong> (remember the “classic” join took about 17 seconds).</p><h2 id=42-theoretical-explanation>4.2. Theoretical Explanation<a hidden class=anchor aria-hidden=true href=#42-theoretical-explanation>#</a></h2><p>This job takes to little time because we’re employing a technique known as a <strong>Broadcast Join</strong>, where the smaller DF, instead of being shuffled causing the bigger DF to be shuffled as well, is being copied entirely to all the executors that are processing the join. So the <strong>bigger DF doesn’t need to be shuffled</strong> again because all the executors have the entire smaller DF to compare to.</p><p>Graphically:</p><p><img loading=lazy src=images/Untitled%208.png alt=Untitled></p><p>The situation when a Small DF and a Big Table join each other is so popular that Spark has implemented a broadcast join detection in the Catalyst Query Optimizer.</p><p>Let’s see in the code this “auto-broadcast detection”.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> bigTable <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>range<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>100000000</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> smallTable <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>range<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>10000</span><span style=color:#f92672>)</span> <span style=color:#75715e>// size estimated by Spark - auto-broadcast
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> joinedNumbers <span style=color:#66d9ef>=</span> bigTable<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>smallTable<span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;id&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>joinedNumbers<span style=color:#f92672>.</span>explain
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled%209.png alt=Untitled></p><p>There is a <code>BroadcastExchange</code> with <code>HashedRelationBroadcastMode</code>, so the smaller DF is being broadcasted before being joined with the bigger one. So, Spark automatically detected that the small table is small enough and employs a <code>BroadcastHashJoin</code> itself. It happens even with the two tables swapped:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> joinedNumbers <span style=color:#66d9ef>=</span> smallTable<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>bigTable<span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;id&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Spark has an automatic range estimator or size estimator that is configured with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.autoBroadcastJoinThreshold&#34;</span><span style=color:#f92672>,</span> <span style=color:#ae81ff>30</span><span style=color:#f92672>)</span> <span style=color:#75715e>// 30 bytes is the upper limit
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.autoBroadcastJoinThreshold&#34;</span><span style=color:#f92672>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span> <span style=color:#75715e>// -1 to deactivate auto-broadcast
</span></span></span></code></pre></div><p>If you put <code>-1</code> you are deactivating auto-broadcast.</p><p>The default value for <code>autoBroadcastJoinThreshold</code> is 10 MB. This configuration is only available for DataFrame API, not for RDD API.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://simdangelo.github.io/tags/coding/>Coding</a></li><li><a href=https://simdangelo.github.io/tags/spark/>Spark</a></li><li><a href=https://simdangelo.github.io/tags/tutorial/>Tutorial</a></li></ul><nav class=paginav><a class=prev href=https://simdangelo.github.io/blog/spark-dataframe-read-write/><span class=title>« Prev</span><br><span>LearningNoteSeries - DataFrame API#1: Read and Write DataFrames</span>
</a><a class=next href=https://simdangelo.github.io/blog/run-spark-application/><span class=title>Next »</span><br><span>LearningNoteSeries - How to run Spark Applications in Scala on your local machine</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://simdangelo.github.io/>SimoneDangelo Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>