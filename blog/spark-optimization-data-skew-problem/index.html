<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem | SimoneDangelo Blog</title>
<meta name=keywords content="coding,spark,tutorial"><meta name=description content="Data Skew in Spark."><meta name=author content="Me"><link rel=canonical href=https://simdangelo.github.io/blog/spark-optimization-data-skew-problem/><link crossorigin=anonymous href=/assets/css/stylesheet.fc10a2e502c8379ca6fd0ccb1371d9d4049c0fd729db19019841464f9733f2ef.css integrity="sha256-/BCi5QLIN5ym/QzLE3HZ1AScD9cp2xkBmEFGT5cz8u8=" rel="preload stylesheet" as=style><link rel=icon href=https://simdangelo.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://simdangelo.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://simdangelo.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://simdangelo.github.io/apple-touch-icon.png><link rel=mask-icon href=https://simdangelo.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://simdangelo.github.io/blog/spark-optimization-data-skew-problem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem"><meta property="og:description" content="Data Skew in Spark."><meta property="og:type" content="article"><meta property="og:url" content="https://simdangelo.github.io/blog/spark-optimization-data-skew-problem/"><meta property="og:image" content="https://simdangelo.github.io/images/papermod-cover.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-06-08T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-08T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://simdangelo.github.io/images/papermod-cover.png"><meta name=twitter:title content="LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem"><meta name=twitter:description content="Data Skew in Spark."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://simdangelo.github.io/blog/"},{"@type":"ListItem","position":2,"name":"LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem","item":"https://simdangelo.github.io/blog/spark-optimization-data-skew-problem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem","name":"LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem","description":"Data Skew in Spark.","keywords":["coding","spark","tutorial"],"articleBody":"0. Resources “Afaque Ahmad” YouTube channel: https://www.youtube.com/@afaqueahmad7117/featured 1. Start the Project If you’re interested in learning how to create a new Spark project in Scala, refer to the initial blog post on Spark available at the following link: https://simdangelo.github.io/blog/run-spark-application/. In this guide, we utilize the same project that was used in previous tutorials and will continue to use in future ones. You can download this project from the following repository: https://github.com/simdangelo/apache-spark-blog-tutorial.\n2. Data Skew 2.1. Data Skew explained When does Data Skew happens?\nLet’s assume a scenario where you have data split in 5 partition (p3 is the largest one, p2 is the smallest one) and an hardware configuration consisting in 1 executors with 5 cores:\nLet’s see how these partitions are processed inside the executor:\nFrom the dotted line on, only one core is working, while all the others are sitting IDLE. This means that there is an uneven utilisation of resources and you’re paying for resources you’re not even using. The ideal scenario should be the following one:\nLet’s try to give a definition to Data Skew: In Apache Spark, the Data Skew problem occurs when the distribution of data across the partitions is uneven. This imbalance can lead to some partitions being much larger than others, causing certain tasks to take significantly longer to process. As a result, the overall job execution time increases, leading to inefficiencies and reduced performance. Data skew typically arises from the presence of highly frequent keys in operations such as joins, groupBy, and aggregations.\nWhat kind of operations can cause Data Skew?\ngroupBy() operation.\nSuppose an operation like:\ndf.groupBy(\"country\").count() If in your dataset there is a country value which is more numerous compared to the others, it turns out that whichever core getting process the partition where that country is present, is going to take a lot of more time compared to the other partitions:\njoin() operation.\nThe reason is the same as grouping operation: there is a massive presence of some specific value of product_id that causes join skew.\nNegative Consequences of Data Skew\nJob is taking time uneven utilization of resources (cores are sitting IDLE as we have seen before) Out Of Memory Errors 2.2. Data Skew example Let’s start by creating a new Scala file called DataSkew and let’s write the usual configuration:\nval spark = SparkSession.builder .appName(\"Data Skew Example\") .master(\"local[4]\") .config(\"spark.sql.adaptive.enabled\", \"false\") .getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1) Two important things to note:\nAdaptive Query Execution (AQE) is one of the possible solution to Data Skew problem, so I disabled that in order to show the problem. Broadcast Join is another possible solution to Data Skew problem, so again I disable that. For this example I used the data that Afaque Ahmad shared in his GitHub Repo (link above). Let’s import a skewed DF, where a specific customer has insanely more transaction than the others, and another DF we’ll join with:\nval transactions_file = \"src/main/resources/data/data_skew/transactions.parquet\" val customer_file = \"src/main/resources/data/data_skew/customers.parquet\" val df_transactions = spark.read.parquet(transactions_file) val df_customers = spark.read.parquet(customer_file) Let’s show the skewness in the df_transactions DF:\nval df_transactions_grouped = df_transactions .groupBy(\"cust_id\") .agg(countDistinct(\"txn_id\").alias(\"ct\")) .orderBy(desc(\"ct\")) This means that, if you join this DF with someone else on cust_id as join key, this join is going to be skewed at that cust_id. So, let’s take a join:\nval df_txn_details = df_transactions.join(df_customers, \"cust_id\", \"inner\") Let’s take an action on df_txn_details in order to start the computation and, in order to keep the SparkUI alive, let’s put as following in the main function:\ndf_txn_details.count() Thread.sleep(1000000) Now, let’s go in the SparkUI where we can look at the Data Skew problem visually:\nLet’s click on the JobId 2 corresponding to the .count() action:\nNow let’s explore the details of StageId 4, that is the one where the join is taken place:\nYou can also look at the Summary Metrics table where it’s clear that the Median Duration of a Task is 34ms, but the max is 4s:\nFrom these pictures, we can clearly observe the Data Skew problem, where parallelism is severely compromised: only one core is active while the others remain IDLE, resulting in parallelism dropping to 1 after a certain point. In contrast, the ideal scenario is where all cores work simultaneously, utilizing parallelism to its fullest extent.\n3. Solutions Solution 1: Adaptive Query Execution (AQE) Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. Statistics can be:\n# of bytes read (basically the size of your input dataset) # of partitions The kind of tuning that AQE gives us is among the following three:\nCoalescing Post Shuffle Partitions. Let’s say you have 200 shuffle partition by default and you have a dataset you want to join on a specific join key and this join key has only 15 distinct values. This means that the remaining remaining 185 partitions are going to be empty. What Spark does is that is going to coalesce all of the partitions into 15 partitions, so that it reduces the number of partitions.\nOptimizing Joins. Spark converts SortMergeJoin (which involves shuffles) into a Broadcast Join BroadcastHashJoin (which does not involve shuffle).\nOptimizing Skew Joins. Spark splits the skewed partition into smaller partitions. It’s written in the documentation:\nTo make this optimization technique effective both of the following configuration must be enabled:\nspark.sql.adaptive.enabled spark.sql.adaptive.skewJoin Code Example Let’s enable AQE by inserting in the code:\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\") // activate AQE spark.conf.set(\"spark.sql.adaptive.skewedJoin.enabled\", \"true\") // activate AQE Then let’s perform again the same join operation as before and look a again at the SparkUI:\nIf you look at the “Tasks” column you can already notice that the total number of stages is much less than before. Let’s look at the details of the JobId 4:\nNow let’s explore the details of StageId 6, in particular the Event Timeline and the Summary Statistics:\nCompared to the previous case, the situation is significantly different.\nthere are only 4 stages now, compared to the 200 stages before; the four stages are executed in roughly the same amount of time, or at least within the same order of magnitude (3 seconds for the shortest, 6 seconds for the longest). In the SparkUI let’s click on the SQL/DataFrame tab to see the query plan and let’s compare the plan without and with AQE.\nWithout AQE:\nWith AQE:\nNote that the two plans are very similar, except for an additional step called AQEShuffleRead, which is introduced when AQE is enabled for each DataFrame performing the join. This is why, with AQE, we have seen only 4 stages instead of 200 in the Event Timeline sections: AQE performed a coalesce operation, reducing the total number of partitions of each DataFrame from 200 to 4. You can verify this by checking the “number of partitions” statistics in these blocks.\nNotice that AQE may not be always the best option.\nSolution 2: Broadcast Join SortMergeJoin is the classical join operation and it assumes that both branches of this joins need to be shuffled and sorted by the column that’s being joined on. So, it’s clear that it involves many operations. Broadcast join, on the other hand, is considered to be better than SortMergeJoin for certain cases, where one of the table is smaller as compared to the other table.\nSortMergeJoin As the name itself suggests this operation involves many different steps:\nshuffling (most expensive operation) sorting merging Shuffling\nLet’s suppose you have 2 dataset (Transactions and Customers) with 3 partitions each and you are doing a join on cust_id and id columns, respectively. In the Transactions the attribute cust_id is abnormally distributed because there’s a specific cust_id that made a lot of transactions.\nAs we already know, to join datasets, the join keys from both tables must be on the same executors, which requires shuffling. Generally when Spark operation performs data shuffling the resulting dataset increases the partition number to 200 by default. For simplicity, let’s assume that the number of shuffle partitions (that you can set with the SQL configuration spark.sql.shuffle.partitions) is 3. So shuffling means transfer of data over the network and this means that values of cust_id and id would end up in different partition.\nWhich is the logic behind a key going into a specific partition? Essentially, how does Spark determine which rows should end up on which executors? By default, Spark uses Hash Partitioning, meaning that the for each key, Spark computes the Hash Code and then applies a modulo operation with the number of partitions. This determines the partition number where the key-value pair should be stored. The formula is:\npartition = hash(key1, key2, ..., key_n) % number_of_shuffle_partitions\nOnly for calculation simplicity, we’ll use:\npartition = key % number_of_shuffle_partitions\nLet’s compute the partition value only for data in Executor 1 (but it’s the same for the other ones) by applying the formula above:\nSorting\nAfter rearranging the rows into new partitions thanks to Hash Partitioning, the join keys in both tables are sorted:\nMerging\nFinally the matching between the two tables in every executor is executed and the SortMergeJoin is done.\nThe final result will be:\nBroadcast Join As we already said in the last post on this blog, in the broadcast join the smaller table will be copied into every executors:\nSo, why Broadcast joins can help to solve Data Skew problem? It’s because Broadcast joins are immune to skewed input dataset and the reason is that you have a complete flexibility to partition in the way you want, so there is no compulsion that you have to partition it by the join key. So what you can do is to perform a repartition(3) to evenly repartition the big table in each executor and then perform a broadcast join. In this case the three executors will work on the same amount of data each.\nIn our code:\nval df_txn_details_broadcasted = df_transactions.join(broadcast(df_customers), \"cust_id\", \"inner\") 3.2 seconds vs 10 seconds of the SortMergeJoin.\nNotice the limitations of broadcast joins: the broadcasted dataset needs to fit in the driver \u0026 executor memory, and if you have many executors, it may take longer than shuffle merge, it could in fact timeout.\nLet’s try our code first by disabling AQE:\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"false\") // disable AQE spark.conf.set(\"spark.sql.adaptive.skewedJoin.enabled\", \"false\") // disable AQE val df_txn_details_broadcasted = df_transactions.join(broadcast(df_customers), \"cust_id\", \"inner\") This is the result:\nThen let’s enabling AQE:\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\") // activate AQE spark.conf.set(\"spark.sql.adaptive.skewedJoin.enabled\", \"true\") // activate AQE val df_txn_details_broadcasted = df_transactions.join(broadcast(df_customers), \"cust_id\", \"inner\") Here’s the result:\nIn this particular scenario, the Broadcast Join performs effectively both with AQE and without it. Indeed, it’s worth noting that the job duration is significantly slower compared to previous cases.\nSolution 3: Salting To Salting basically refers to adding randomness to a key in order to be able to evenly distribute it.\nSuppose you have a Table1 (bigger one, the skewed dataset) and Table2 (smallest one) that you want to join on id column. This column on Table1 has 1 million rows with value 1, 5 rows with value 2, and 10 rows with value 3. It’s of course very skewed. If we were to perform a classical SortMergeJoin and we set the number of shuffle partition to 3, it happens that in the first partition there would be 1 million elements, in the second partition 5 elements, and in the third partition 10 elements because of the Hash Partitioning formula partition=hash(value) % number_of_shuffle_partitions (we’ll use a simplified version of that formula):\n1%3=1: 1 (count=1.000.000) goes into partition 1 2%3=2: 2 (count=5) goes into partition 2 3%3=0: 3 (count=10) goes into partition 0 This third solution to Data Skew problem is called Salt Technique.\nStep 1: Choose a salt number It indicates how much we want to distribute the data. In our case let’s choose a number between 0 included and 3 not included [0, 3) and let’s add this new column to each of the rows:\nNow, instead of doing the join on id columns, we’ll do the join on (id, salt). Since this becomes the new join key, the Hash Partitioning formula that decides which row is going to which partition becomes:\nhash(id, salt) % 3\nThe advantage of this is that, while without salting we had hash(1) % 3 = N and so N remained the same for every rows with id equal to 1 (implying that every rows with id==1 go to the same partition), now we’ll have 3 different hashes for id==1:\npartition = hash(1, 0) % 3 = 0\npartition = hash(1, 1) % 3 = 1\npartition = hash(1, 2) % 3 = 2\nSo now we’re able to distribute 1s id values more effectively into 3 different partitions that appear more balanced with a distribution similar to the following one:\nRemember the key point of this step: we have just created a new join key that it’s no longer composed by id column, but it’s now composed of (id, salt). This implies that the Hash formula is no longer computed on id only, but on (id, salt).\nStep 2: Add Salt List for each key of Table2: Since we have just created a new join key composed of (id, salt), we now need a way to create this new join key in Table2 as well.\nSo, take Table2 and append all the salt values to the id column as a list:\nStep 3: explode Salt List to get Salt for each Key The Spark explode function generates a new row for each element in an array. In this context, we will use the explode function on the salt_list column, resulting in three new rows for each id value, each corresponding to one of the values contained in the salt_list. Let’s see it graphically:\nThe result is that for Table2, we now have a new join key composed of the (id, salt) columns, making it ready for the join operation with Table1.\nStep 4: Make the Join Now, you can easily perform the join operation without encountering skewed data. As shown in the following image, rows with id==1 are no longer confined to the same partition; instead, they are distributed across all partitions. I want repeat the reason again: that’s because the Hash formula is no longer computed on id only, but on (id, salt).\nCode Example spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\") spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\") // Create df_uniform DataFrame val df_uniform = spark.createDataFrame((0 until 1000000).map(Tuple1(_))).toDF(\"value\") // Create skewed DataFrames val df0 = spark.createDataFrame(Seq.fill(999990)(0).map(Tuple1(_))).toDF(\"value\").repartition(1) val df1 = spark.createDataFrame(Seq.fill(15)(1).map(Tuple1(_))).toDF(\"value\").repartition(1) val df2 = spark.createDataFrame(Seq.fill(10)(2).map(Tuple1(_))).toDF(\"value\").repartition(1) val df3 = spark.createDataFrame(Seq.fill(5)(3).map(Tuple1(_))).toDF(\"value\").repartition(1) // Union skewed DataFrames val df_skew = df0.union(df1).union(df2).union(df3) // Add partition column and show partition counts val df_skew_show = df_skew .withColumn(\"partition\", spark_partition_id()) .groupBy(\"partition\") .count() .orderBy(\"partition\") print(\"Data distribution across Partitions before Salting Technique:\\n\") df_skew_show.show() val SALT_NUMBER = spark.conf.get(\"spark.sql.shuffle.partitions\").toInt // Add salt column to df_skew val df_skew_with_salt = df_skew.withColumn(\"salt\", (rand() * SALT_NUMBER).cast(IntegerType)) // Add salt_values and explode to create salt column in df_uniform val saltValues = (0 until SALT_NUMBER).map(lit(_)).toArray val df_uniform_with_salt = df_uniform .withColumn(\"salt_values\", array(saltValues: _*)) .withColumn(\"salt\", explode(col(\"salt_values\"))) // Perform the join operation between df_skew_with_salt and df_uniform_with_salt val df_joined = df_skew_with_salt.join(df_uniform_with_salt, Seq(\"value\", \"salt\"), \"inner\") // Add partition column and show partition counts val df_joined_show = df_joined .withColumn(\"partition\", spark_partition_id()) .groupBy(\"partition\") .count() .orderBy(\"partition\") print(\"Data distribution across Partitions after Salting Technique:\\n\") df_joined_show.show() Here’s the result:\nSee how the data is now distributed evenly across all partitions.\n","wordCount":"2547","inLanguage":"en","image":"https://simdangelo.github.io/images/papermod-cover.png","datePublished":"2024-06-08T00:00:00Z","dateModified":"2024-06-08T00:00:00Z","author":[{"@type":"Person","name":"Me"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://simdangelo.github.io/blog/spark-optimization-data-skew-problem/"},"publisher":{"@type":"Organization","name":"SimoneDangelo Blog","logo":{"@type":"ImageObject","url":"https://simdangelo.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://simdangelo.github.io/ accesskey=h title="SimoneDangelo Blog (Alt + H)">SimoneDangelo Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://simdangelo.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://simdangelo.github.io/search/ title=Search><span>Search</span></a></li><li><a href=https://simdangelo.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://github.com/simdangelo title=GitHub><span>GitHub</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://simdangelo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://simdangelo.github.io/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">LearningNoteSeries - SparkOptimization#2: How to solve Data Skew problem</h1><div class=post-meta><span title='2024-06-08 00:00:00 +0000 UTC'>June 8, 2024</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Me</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#0-resources aria-label="0. Resources">0. Resources</a></li><li><a href=#1-start-the-project aria-label="1. Start the Project">1. Start the Project</a></li><li><a href=#2-data-skew aria-label="2. Data Skew">2. Data Skew</a><ul><li><a href=#21-data-skew-explained aria-label="2.1. Data Skew explained">2.1. Data Skew explained</a></li><li><a href=#22-data-skew-example aria-label="2.2. Data Skew example">2.2. Data Skew example</a></li></ul></li><li><a href=#3-solutions aria-label="3. Solutions">3. Solutions</a><ul><li><a href=#solution-1-adaptive-query-execution-aqe aria-label="Solution 1: Adaptive Query Execution (AQE)">Solution 1: Adaptive Query Execution (AQE)</a><ul><li><a href=#code-example aria-label="Code Example">Code Example</a></li></ul></li><li><a href=#solution-2-broadcast-join aria-label="Solution 2: Broadcast Join">Solution 2: Broadcast Join</a><ul><li><a href=#sortmergejoin aria-label=SortMergeJoin>SortMergeJoin</a></li><li><a href=#broadcast-join aria-label="Broadcast Join">Broadcast Join</a></li></ul></li><li><a href=#solution-3-salting aria-label="Solution 3: Salting">Solution 3: Salting</a><ul><li><a href=#step-1-choose-a-salt-number aria-label="Step 1: Choose a salt number">Step 1: Choose a salt number</a></li><li><a href=#step-2-add-salt-list-for-each-key-of-table2 aria-label="Step 2: Add Salt List for each key of Table2:">Step 2: Add Salt List for each key of Table2:</a></li><li><a href=#step-3-explode-salt-list-to-get-salt-for-each-key aria-label="Step 3: explode Salt List to get Salt for each Key">Step 3: explode Salt List to get Salt for each Key</a></li><li><a href=#step-4-make-the-join aria-label="Step 4: Make the Join">Step 4: Make the Join</a></li><li><a href=#code-example-1 aria-label="Code Example">Code Example</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=0-resources>0. Resources<a hidden class=anchor aria-hidden=true href=#0-resources>#</a></h1><ul><li>&ldquo;Afaque Ahmad&rdquo; YouTube channel: <a href=https://www.youtube.com/@afaqueahmad7117/featured>https://www.youtube.com/@afaqueahmad7117/featured</a></li></ul><h1 id=1-start-the-project>1. Start the Project<a hidden class=anchor aria-hidden=true href=#1-start-the-project>#</a></h1><p>If you’re interested in learning how to create a new Spark project in Scala, refer to the initial blog post on Spark available at the following link: <a href=https://simdangelo.github.io/blog/run-spark-application/>https://simdangelo.github.io/blog/run-spark-application/</a>. In this guide, we utilize the same project that was used in previous tutorials and will continue to use in future ones. You can download this project from the following repository: <a href=https://github.com/simdangelo/apache-spark-blog-tutorial>https://github.com/simdangelo/apache-spark-blog-tutorial</a>.</p><h1 id=2-data-skew>2. Data Skew<a hidden class=anchor aria-hidden=true href=#2-data-skew>#</a></h1><h2 id=21-data-skew-explained>2.1. Data Skew explained<a hidden class=anchor aria-hidden=true href=#21-data-skew-explained>#</a></h2><p><strong>When does Data Skew happens?</strong></p><p>Let’s assume a scenario where you have data split in <strong>5 partition</strong> (p3 is the largest one, p2 is the smallest one) and an hardware configuration consisting in <strong>1 executors</strong> with <strong>5 cores</strong>:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-6.drawio.svg alt="Untitled Diagram-Page-6.drawio.svg"></p><p>Let’s see how these partitions are processed inside the executor:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-7.drawio.svg alt="Untitled Diagram-Page-7.drawio.svg"></p><p>From the dotted line on, only one core is working, while all the others are sitting <strong>IDLE</strong>. This means that there is an <strong>uneven utilisation of resources</strong> and you’re paying for resources you’re not even using. The ideal scenario should be the following one:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-7.drawio_%282%29.svg alt="Untitled Diagram-Page-7.drawio (2).svg"></p><p>Let’s try to give a definition to <strong>Data Skew</strong>: In Apache Spark, the Data Skew problem occurs when the distribution of data across the partitions is uneven. This imbalance can lead to some partitions being much larger than others, causing certain tasks to take significantly longer to process. As a result, the overall job execution time increases, leading to inefficiencies and reduced performance. Data skew typically arises from the presence of highly frequent keys in operations such as joins, groupBy, and aggregations.</p><p><strong>What kind of operations can cause Data Skew?</strong></p><ul><li><p><code>groupBy()</code> operation.</p><p>Suppose an operation like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;country&#34;</span><span style=color:#f92672>).</span>count<span style=color:#f92672>()</span>
</span></span></code></pre></div><p>If in your dataset there is a <code>country</code> value which is more numerous compared to the others, it turns out that whichever core getting process the partition where that country is present, is going to take a lot of more time compared to the other partitions:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-7.drawio_%285%29.svg alt="Untitled Diagram-Page-7.drawio (5).svg"></p></li><li><p><code>join()</code> operation.</p><p>The reason is the same as grouping operation: there is a massive presence of some specific value of <code>product_id</code> that causes <code>join skew</code>.</p></li></ul><p><strong>Negative Consequences of Data Skew</strong></p><ul><li>Job is taking time</li><li>uneven utilization of resources (cores are sitting IDLE as we have seen before)</li><li>Out Of Memory Errors</li></ul><h2 id=22-data-skew-example>2.2. Data Skew example<a hidden class=anchor aria-hidden=true href=#22-data-skew-example>#</a></h2><p>Let’s start by creating a new Scala file called DataSkew and let’s write the usual configuration:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> spark <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>SparkSession</span><span style=color:#f92672>.</span>builder
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>appName<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Data Skew Example&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>master<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;local[4]&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>config<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;false&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>getOrCreate<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sparkContext<span style=color:#f92672>.</span>setLogLevel<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;WARN&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.autoBroadcastJoinThreshold&#34;</span><span style=color:#f92672>,</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Two important things to note:</p><ul><li><strong>Adaptive Query Execution (AQE)</strong> is one of the possible solution to Data Skew problem, so I disabled that in order to show the problem.</li><li>Broadcast Join is another possible solution to Data Skew problem, so again I disable that.</li></ul><p>For this example I used the data that Afaque Ahmad shared in his GitHub Repo (link above). Let’s import a skewed DF, where a specific customer has insanely more transaction than the others, and another DF we’ll join with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> transactions_file <span style=color:#66d9ef>=</span> <span style=color:#e6db74>&#34;src/main/resources/data/data_skew/transactions.parquet&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> customer_file <span style=color:#66d9ef>=</span> <span style=color:#e6db74>&#34;src/main/resources/data/data_skew/customers.parquet&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_transactions <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet<span style=color:#f92672>(</span>transactions_file<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_customers <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet<span style=color:#f92672>(</span>customer_file<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Let’s show the skewness in the <code>df_transactions</code> DF:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df_transactions_grouped <span style=color:#66d9ef>=</span> df_transactions
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;cust_id&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>agg<span style=color:#f92672>(</span>countDistinct<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;txn_id&#34;</span><span style=color:#f92672>).</span>alias<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;ct&#34;</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span>desc<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;ct&#34;</span><span style=color:#f92672>))</span>
</span></span></code></pre></div><p><img loading=lazy src=images/Untitled.png alt=Untitled></p><p>This means that, if you join this DF with someone else on <code>cust_id</code> as join key, this join is going to be skewed at that <code>cust_id</code>. So, let’s take a join:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df_txn_details <span style=color:#66d9ef>=</span> df_transactions<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>df_customers<span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;cust_id&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;inner&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Let’s take an action on <code>df_txn_details</code> in order to start the computation and, in order to keep the SparkUI alive, let’s put as following in the <code>main</code> function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>df_txn_details<span style=color:#f92672>.</span>count<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Thread</span><span style=color:#f92672>.</span>sleep<span style=color:#f92672>(</span><span style=color:#ae81ff>1000000</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Now, let’s go in the SparkUI where we can look at the Data Skew problem visually:</p><p><img loading=lazy src=images/Untitled%201.png alt=Untitled></p><p>Let’s click on the JobId 2 corresponding to the <code>.count()</code> action:</p><p><img loading=lazy src=images/Untitled%202.png alt=Untitled></p><p>Now let’s explore the details of StageId 4, that is the one where the join is taken place:</p><p><img loading=lazy src=images/Untitled%203.png alt=Untitled></p><p>You can also look at the Summary Metrics table where it’s clear that the Median Duration of a Task is 34ms, but the max is 4s:</p><p><img loading=lazy src=images/Untitled%204.png alt=Untitled></p><p>From these pictures, we can clearly observe the <strong>Data Skew problem</strong>, where parallelism is severely compromised: only one core is active while the others remain IDLE, resulting in parallelism dropping to 1 after a certain point. In contrast, the ideal scenario is where all cores work simultaneously, utilizing parallelism to its fullest extent.</p><h1 id=3-solutions>3. Solutions<a hidden class=anchor aria-hidden=true href=#3-solutions>#</a></h1><h2 id=solution-1-adaptive-query-execution-aqe>Solution 1: <strong>Adaptive Query Execution (AQE)</strong><a hidden class=anchor aria-hidden=true href=#solution-1-adaptive-query-execution-aqe>#</a></h2><p><strong>Adaptive Query Execution (AQE)</strong> is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. Statistics can be:</p><ul><li># of bytes read (basically the size of your input dataset)</li><li># of partitions</li></ul><p>The kind of tuning that AQE gives us is among the following three:</p><ul><li><p><strong>Coalescing Post Shuffle Partitions</strong>. Let’s say you have 200 shuffle partition by default and you have a dataset you want to join on a specific join key and this join key has only 15 distinct values. This means that the remaining remaining 185 partitions are going to be empty. What Spark does is that is going to coalesce all of the partitions into 15 partitions, so that it reduces the number of partitions.</p></li><li><p><strong>Optimizing Joins</strong>. Spark converts <code>SortMergeJoin</code> (which involves shuffles) into a Broadcast Join <code>BroadcastHashJoin</code> (which does not involve shuffle).</p></li><li><p><strong>Optimizing Skew Joins</strong>. Spark splits the skewed partition into smaller partitions. It’s written in the documentation:</p><p><img loading=lazy src=images/Untitled%205.png alt=Untitled></p><p>To make this optimization technique effective both of the following configuration must be enabled:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>adaptive<span style=color:#f92672>.</span>enabled
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql<span style=color:#f92672>.</span>adaptive<span style=color:#f92672>.</span>skewJoin
</span></span></code></pre></div></li></ul><h3 id=code-example><strong>Code Example</strong><a hidden class=anchor aria-hidden=true href=#code-example>#</a></h3><p>Let’s enable AQE by inserting in the code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// activate AQE
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.skewedJoin.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// activate AQE
</span></span></span></code></pre></div><p>Then let’s perform again the same join operation as before and look a again at the SparkUI:</p><p><img loading=lazy src=images/Untitled%206.png alt=Untitled></p><p>If you look at the “Tasks” column you can already notice that the total number of stages is much less than before. Let’s look at the details of the JobId 4:</p><p><img loading=lazy src=images/Untitled%207.png alt=Untitled></p><p>Now let’s explore the details of StageId 6, in particular the Event Timeline and the Summary Statistics:</p><p><img loading=lazy src=images/Untitled%208.png alt=Untitled></p><p>Compared to the previous case, the situation is significantly different.</p><ul><li>there are only 4 stages now, compared to the 200 stages before;</li><li>the four stages are executed in roughly the same amount of time, or at least within the same order of magnitude (3 seconds for the shortest, 6 seconds for the longest).</li></ul><p>In the SparkUI let’s click on the <strong>SQL/DataFrame</strong> tab to see the query plan and let’s compare the plan without and with AQE.</p><p>Without AQE:</p><p><img loading=lazy src=images/Untitled%209.png alt=Untitled.png></p><p>With AQE:</p><p><img loading=lazy src=images/Untitled%2010.png alt=Untitled.png></p><p>Note that the two plans are very similar, except for an additional step called <code>AQEShuffleRead</code>, which is introduced when AQE is enabled for each DataFrame performing the join. This is why, with AQE, we have seen only 4 stages instead of 200 in the Event Timeline sections: AQE performed a coalesce operation, reducing the total number of partitions of each DataFrame from <strong>200 to 4</strong>. You can verify this by checking the “<strong>number of partitions”</strong> statistics in these blocks.</p><p>Notice that AQE may not be always the best option.</p><h2 id=solution-2-broadcast-join>Solution 2: Broadcast Join<a hidden class=anchor aria-hidden=true href=#solution-2-broadcast-join>#</a></h2><p><code>SortMergeJoin</code> is the classical join operation and it assumes that both branches of this joins need to be shuffled and sorted by the column that’s being joined on. So, it’s clear that it involves many operations. Broadcast join, on the other hand, is considered to be better than <code>SortMergeJoin</code> for certain cases, where one of the table is smaller as compared to the other table.</p><h3 id=sortmergejoin><code>SortMergeJoin</code><a hidden class=anchor aria-hidden=true href=#sortmergejoin>#</a></h3><p>As the name itself suggests this operation involves many different steps:</p><ul><li>shuffling (most expensive operation)</li><li>sorting</li><li>merging</li></ul><p><strong>Shuffling</strong></p><p>Let’s suppose you have 2 dataset (<code>Transactions</code> and <code>Customers</code>) with 3 partitions each and you are doing a join on <code>cust_id</code> and <code>id</code> columns, respectively. In the <code>Transactions</code> the attribute <code>cust_id</code> is abnormally distributed because there’s a specific <code>cust_id</code> that made a lot of transactions.</p><p><img loading=lazy src=images/Untitled_Diagram-Page-1.drawio.svg alt="Untitled Diagram-Page-1.drawio.svg"></p><p>As we already know, to join datasets, the join keys from both tables must be on the same executors, which requires shuffling. Generally when Spark operation performs data shuffling the resulting dataset increases the partition number to <strong>200 by default</strong>. For simplicity, let’s assume that the number of shuffle partitions (that you can set with the SQL configuration <code>spark.sql.shuffle.partitions</code>) is 3. So shuffling means transfer of data over the network and this means that values of <code>cust_id</code> and <code>id</code> would end up in different partition.</p><p>Which is the logic behind a key going into a specific partition? Essentially, how does Spark determine which rows should end up on which executors? By default, Spark uses <strong>Hash Partitioning</strong>, meaning that the for each key, Spark computes the Hash Code and then applies a modulo operation with the number of partitions. This determines the partition number where the key-value pair should be stored. The formula is:</p><p><code>partition = hash(key1, key2, ..., key_n) % number_of_shuffle_partitions</code></p><p>Only for calculation simplicity, we’ll use:</p><p><code>partition = key % number_of_shuffle_partitions</code></p><p>Let’s compute the <code>partition</code> value only for data in Executor 1 (but it’s the same for the other ones) by applying the formula above:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-2.drawio.svg alt="Untitled Diagram-Page-2.drawio.svg"></p><p><strong>Sorting</strong></p><p>After rearranging the rows into new partitions thanks to Hash Partitioning, the join keys in both tables are sorted:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-3.drawio_%281%29.svg alt="Untitled Diagram-Page-3.drawio (1).svg"></p><p><strong>Merging</strong></p><p>Finally the matching between the two tables in every executor is executed and the <code>SortMergeJoin</code> is done.</p><p>The final result will be:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-4.drawio.svg alt="Untitled Diagram-Page-4.drawio.svg"></p><h3 id=broadcast-join>Broadcast Join<a hidden class=anchor aria-hidden=true href=#broadcast-join>#</a></h3><p>As we already said in the last post on this blog, in the broadcast join the smaller table will be copied into every executors:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-5.drawio.svg alt="Untitled Diagram-Page-5.drawio.svg"></p><p>So, why Broadcast joins can help to solve <strong>Data Skew</strong> problem? It’s because Broadcast joins are immune to skewed input dataset and the reason is that you have a complete flexibility to partition in the way you want, so there is no compulsion that you have to partition it by the join key. So what you can do is to perform a <code>repartition(3)</code> to <strong>evenly repartition the big table in each executor</strong> and then perform a <strong>broadcast join</strong>. In this case the three executors will work on the same amount of data each.</p><p>In our code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#66d9ef>val</span> df_txn_details_broadcasted <span style=color:#66d9ef>=</span> df_transactions<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>broadcast<span style=color:#f92672>(</span>df_customers<span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;cust_id&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;inner&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>3.2 seconds vs 10 seconds of the <code>SortMergeJoin.</code></p><p>Notice the <strong>limitations of broadcast joins</strong>: the broadcasted dataset needs to fit in the driver & executor memory, and if you have many executors, it may take longer than shuffle merge, it could in fact timeout.</p><p>Let’s try our code first by disabling AQE:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;false&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// disable AQE
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.skewedJoin.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;false&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// disable AQE
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_txn_details_broadcasted <span style=color:#66d9ef>=</span> df_transactions<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>broadcast<span style=color:#f92672>(</span>df_customers<span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;cust_id&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;inner&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>This is the result:</p><p><img loading=lazy src=images/Untitled%2011.png alt=Untitled></p><p>Then let’s enabling AQE:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// activate AQE
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.skewedJoin.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;true&#34;</span><span style=color:#f92672>)</span> <span style=color:#75715e>// activate AQE
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_txn_details_broadcasted <span style=color:#66d9ef>=</span> df_transactions<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>broadcast<span style=color:#f92672>(</span>df_customers<span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;cust_id&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;inner&#34;</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2012.png alt=Untitled></p><p>In this particular scenario, the <strong>Broadcast Join</strong> performs effectively both with AQE and without it. Indeed, it&rsquo;s worth noting that the job duration is significantly slower compared to previous cases.</p><h2 id=solution-3-salting>Solution 3: Salting<a hidden class=anchor aria-hidden=true href=#solution-3-salting>#</a></h2><p>To <strong>Salting</strong> basically refers to adding randomness to a key in order to be able to evenly distribute it.</p><p>Suppose you have a <strong>Table1</strong> (bigger one, the skewed dataset) and <strong>Table2</strong> (smallest one) that you want to join on <code>id</code> column. This column on Table1 has 1 million rows with value 1, 5 rows with value 2, and 10 rows with value 3. It’s of course very skewed. If we were to perform a classical <code>SortMergeJoin</code> and we set the number of shuffle partition to 3, it happens that in the first partition there would be 1 million elements, in the second partition 5 elements, and in the third partition 10 elements because of the Hash Partitioning formula <code>partition=hash(value) % number_of_shuffle_partitions</code> (we’ll use a simplified version of that formula):</p><ul><li><code>1%3=1</code>: <code>1</code> (count=1.000.000) goes into partition 1</li><li><code>2%3=2</code>: <code>2</code> (count=5) goes into partition 2</li><li><code>3%3=0</code>: <code>3</code> (count=10) goes into partition 0</li></ul><p><img loading=lazy src=images/Untitled_Diagram-Page-8.drawio_%284%29.svg alt="Untitled Diagram-Page-8.drawio (4).svg"></p><p>This third solution to Data Skew problem is called <strong>Salt Technique</strong>.</p><h3 id=step-1-choose-a-salt-number>Step 1: <strong>Choose a salt number</strong><a hidden class=anchor aria-hidden=true href=#step-1-choose-a-salt-number>#</a></h3><p>It indicates how much we want to distribute the data. In our case let’s choose a number between 0 included and 3 not included <code>[0, 3)</code> and let’s add this new column to each of the rows:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-9.drawio_%282%29.svg alt="Untitled Diagram-Page-9.drawio (2).svg"></p><p>Now, instead of doing the join on <code>id</code> columns, we’ll do the join on <code>(id, salt)</code>. Since this becomes the new join key, the Hash Partitioning formula that decides which row is going to which partition becomes:</p><p><code>hash(id, salt) % 3</code></p><p>The advantage of this is that, while without salting we had <code>hash(1) % 3 = N</code> and so N remained the same for every rows with <code>id</code> equal to <code>1</code> (implying that every rows with <code>id==1</code> go to the same partition), now we’ll have 3 different hashes for <code>id==1</code>:</p><p><code>partition = hash(1, 0) % 3 = 0</code></p><p><code>partition = hash(1, 1) % 3 = 1</code></p><p><code>partition = hash(1, 2) % 3 = 2</code></p><p>So now we’re able to distribute <code>1</code>s <code>id</code> values more effectively into 3 different partitions that appear more balanced with a distribution similar to the following one:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-8.drawio_%285%29.svg alt="Untitled Diagram-Page-8.drawio (5).svg"></p><p>Remember the key point of this step: we have just <strong>created a new join key</strong> that it’s no longer composed by <code>id</code> column, but it’s now composed of <code>(id, salt)</code>. This implies that the Hash formula is no longer computed on <code>id</code> only, but on <code>(id, salt)</code>.</p><h3 id=step-2-add-salt-list-for-each-key-of-table2>Step 2: <strong>Add Salt List for each key of Table2</strong>:<a hidden class=anchor aria-hidden=true href=#step-2-add-salt-list-for-each-key-of-table2>#</a></h3><p>Since we have just created a new join key composed of <code>(id, salt)</code>, we now need a way to create this new join key in Table2 as well.</p><p>So, take Table2 and append all the salt values to the <code>id</code> column as a list:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-9.drawio_%287%29.svg alt="Untitled Diagram-Page-9.drawio (7).svg"></p><h3 id=step-3-explode-salt-list-to-get-salt-for-each-key>Step 3: <strong><code>explode</code> Salt List to get Salt for each Key</strong><a hidden class=anchor aria-hidden=true href=#step-3-explode-salt-list-to-get-salt-for-each-key>#</a></h3><p>The Spark <code>explode</code> function generates a new row for each element in an array. In this context, we will use the <code>explode</code> function on the <code>salt_list</code> column, resulting in three new rows for each <code>id</code> value, each corresponding to one of the values contained in the <code>salt_list</code>. Let’s see it graphically:</p><p><img loading=lazy src=images/Untitled_Diagram-Page-9.drawio_%288%29.svg alt="Untitled Diagram-Page-9.drawio (8).svg"></p><p>The result is that for Table2, we now have a new join key composed of the <code>(id, salt)</code> columns, making it ready for the join operation with Table1.</p><h3 id=step-4-make-the-join>Step 4: Make <strong>the Join</strong><a hidden class=anchor aria-hidden=true href=#step-4-make-the-join>#</a></h3><p>Now, you can easily perform the join operation without encountering skewed data. As shown in the following image, rows with <code>id==1</code> are no longer confined to the same partition; instead, they are distributed across all partitions. I want repeat the reason again: that’s because the Hash formula is no longer computed on <code>id</code> only, but on <code>(id, salt)</code>.</p><p><img loading=lazy src=images/Untitled_Diagram-Page-10.drawio.svg alt="Untitled Diagram-Page-10.drawio.svg"></p><h3 id=code-example-1><strong>Code Example</strong><a hidden class=anchor aria-hidden=true href=#code-example-1>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.shuffle.partitions&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;3&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>set<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.adaptive.enabled&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;false&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Create df_uniform DataFrame
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_uniform <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>((</span><span style=color:#ae81ff>0</span> until <span style=color:#ae81ff>1000000</span><span style=color:#f92672>).</span>map<span style=color:#f92672>(</span><span style=color:#a6e22e>Tuple1</span><span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>))).</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Create skewed DataFrames
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df0 <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>(</span><span style=color:#a6e22e>Seq</span><span style=color:#f92672>.</span>fill<span style=color:#f92672>(</span><span style=color:#ae81ff>999990</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>0</span><span style=color:#f92672>).</span>map<span style=color:#f92672>(</span><span style=color:#a6e22e>Tuple1</span><span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>))).</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value&#34;</span><span style=color:#f92672>).</span>repartition<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df1 <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>(</span><span style=color:#a6e22e>Seq</span><span style=color:#f92672>.</span>fill<span style=color:#f92672>(</span><span style=color:#ae81ff>15</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>).</span>map<span style=color:#f92672>(</span><span style=color:#a6e22e>Tuple1</span><span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>))).</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value&#34;</span><span style=color:#f92672>).</span>repartition<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df2 <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>(</span><span style=color:#a6e22e>Seq</span><span style=color:#f92672>.</span>fill<span style=color:#f92672>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>2</span><span style=color:#f92672>).</span>map<span style=color:#f92672>(</span><span style=color:#a6e22e>Tuple1</span><span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>))).</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value&#34;</span><span style=color:#f92672>).</span>repartition<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df3 <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>createDataFrame<span style=color:#f92672>(</span><span style=color:#a6e22e>Seq</span><span style=color:#f92672>.</span>fill<span style=color:#f92672>(</span><span style=color:#ae81ff>5</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>3</span><span style=color:#f92672>).</span>map<span style=color:#f92672>(</span><span style=color:#a6e22e>Tuple1</span><span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>))).</span>toDF<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value&#34;</span><span style=color:#f92672>).</span>repartition<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Union skewed DataFrames
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_skew <span style=color:#66d9ef>=</span> df0<span style=color:#f92672>.</span>union<span style=color:#f92672>(</span>df1<span style=color:#f92672>).</span>union<span style=color:#f92672>(</span>df2<span style=color:#f92672>).</span>union<span style=color:#f92672>(</span>df3<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Add partition column and show partition counts
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_skew_show <span style=color:#66d9ef>=</span> df_skew
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;partition&#34;</span><span style=color:#f92672>,</span> spark_partition_id<span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;partition&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>count<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;partition&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Data distribution across Partitions before Salting Technique:\n&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>df_skew_show<span style=color:#f92672>.</span>show<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> <span style=color:#a6e22e>SALT_NUMBER</span> <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>conf<span style=color:#f92672>.</span>get<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;spark.sql.shuffle.partitions&#34;</span><span style=color:#f92672>).</span>toInt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Add salt column to df_skew
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_skew_with_salt <span style=color:#66d9ef>=</span> df_skew<span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;salt&#34;</span><span style=color:#f92672>,</span> <span style=color:#f92672>(</span>rand<span style=color:#f92672>()</span> <span style=color:#f92672>*</span> <span style=color:#a6e22e>SALT_NUMBER</span><span style=color:#f92672>).</span>cast<span style=color:#f92672>(</span><span style=color:#a6e22e>IntegerType</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Add salt_values and explode to create salt column in df_uniform
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> saltValues <span style=color:#66d9ef>=</span> <span style=color:#f92672>(</span><span style=color:#ae81ff>0</span> until <span style=color:#a6e22e>SALT_NUMBER</span><span style=color:#f92672>).</span>map<span style=color:#f92672>(</span>lit<span style=color:#f92672>(</span><span style=color:#66d9ef>_</span><span style=color:#f92672>)).</span>toArray
</span></span><span style=display:flex><span><span style=color:#66d9ef>val</span> df_uniform_with_salt <span style=color:#66d9ef>=</span> df_uniform
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;salt_values&#34;</span><span style=color:#f92672>,</span> array<span style=color:#f92672>(</span>saltValues<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>_</span><span style=color:#66d9ef>*</span><span style=color:#f92672>))</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;salt&#34;</span><span style=color:#f92672>,</span> explode<span style=color:#f92672>(</span>col<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;salt_values&#34;</span><span style=color:#f92672>)))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Perform the join operation between df_skew_with_salt and df_uniform_with_salt
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_joined <span style=color:#66d9ef>=</span> df_skew_with_salt<span style=color:#f92672>.</span>join<span style=color:#f92672>(</span>df_uniform_with_salt<span style=color:#f92672>,</span> <span style=color:#a6e22e>Seq</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;value&#34;</span><span style=color:#f92672>,</span> <span style=color:#e6db74>&#34;salt&#34;</span><span style=color:#f92672>),</span> <span style=color:#e6db74>&#34;inner&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Add partition column and show partition counts
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>val</span> df_joined_show <span style=color:#66d9ef>=</span> df_joined
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>withColumn<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;partition&#34;</span><span style=color:#f92672>,</span> spark_partition_id<span style=color:#f92672>())</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>groupBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;partition&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>count<span style=color:#f92672>()</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>.</span>orderBy<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;partition&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Data distribution across Partitions after Salting Technique:\n&#34;</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>df_joined_show<span style=color:#f92672>.</span>show<span style=color:#f92672>()</span>
</span></span></code></pre></div><p>Here’s the result:</p><p><img loading=lazy src=images/Untitled%2013.png alt=Untitled></p><p>See how the data is now distributed evenly across all partitions.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://simdangelo.github.io/tags/coding/>Coding</a></li><li><a href=https://simdangelo.github.io/tags/spark/>Spark</a></li><li><a href=https://simdangelo.github.io/tags/tutorial/>Tutorial</a></li></ul><nav class=paginav><a class=prev href=https://simdangelo.github.io/blog/presentation/><span class=title>« Prev</span><br><span># Presentation (pinned)</span>
</a><a class=next href=https://simdangelo.github.io/blog/spark-optimization-broadcast-join/><span class=title>Next »</span><br><span>LearningNoteSeries - Spark Optimization#1: Broadcast Joins</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://simdangelo.github.io/>SimoneDangelo Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>